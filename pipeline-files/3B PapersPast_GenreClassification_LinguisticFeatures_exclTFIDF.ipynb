{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Papers Past Genre Classification\n",
    "# Notebook 3B: Linguistic Features and Text Statistics (excluding TF-IDF)\n",
    "---\n",
    "\n",
    "This notebook reads in a Pandas dataframe saved as a pickle (the output of Notebook 2: Labelling) and uses spaCy, textstat, and textfeatures to add columns of linguistic features. This version of the notebook does not include the TF-IDF feature (the TF-IDF feature provides a more precise model but is computationally expensive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Owner\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from datetime import date\n",
    "from datetime import datetime\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "import spacy\n",
    "import math\n",
    "import textstat\n",
    "import textfeatures as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n"
     ]
    }
   ],
   "source": [
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '20220110_PP_4628articles_labelled.pkl'\n",
    "input_df = pd.read_pickle(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The genre label 'Other' includes anything that did not easily fit in one of the other genre categories, including articles that couldn't be classified due to high number of OCR errors. For efficiency, articles labelled as 'Other' are removed before linguistic feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3518"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_df = input_df[input_df.genre != \"Other\"]\n",
    "len(input_df )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text column contains unnecessary symbols that are the result of OCR errors. The cleaner function cleans the text using a regular expression that parses only alphanumeric strings and hyphens (to include hyphenated words). The basis of this code is sourced from: https://prrao87.github.io/blog/spacy/nlp/performance/2020/05/02/spacy-multiprocess.html. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaner(df, column_name):\n",
    "    \"\"\"\n",
    "    Remove unnecessary symbols to create a clean text column from the original dataframe column using a regex.\n",
    "    \"\"\"\n",
    "    # A column of sentence count is added to the dataframe before punctuation is removed.\n",
    "    df['sentence_count'] = df[column_name].apply(lambda x: textstat.sentence_count(x))\n",
    "\n",
    "    # Regex pattern for only alphanumeric, hyphenated text\n",
    "    pattern = re.compile(r\"[A-Za-z0-9\\-]{1,50}\")\n",
    "    df['clean_text'] = df[column_name].str.findall(pattern).str.join(' ')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = cleaner(input_df, 'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>newspaper_id</th>\n",
       "      <th>newspaper</th>\n",
       "      <th>article_id</th>\n",
       "      <th>avg_line_width</th>\n",
       "      <th>min_line_width</th>\n",
       "      <th>max_line_width</th>\n",
       "      <th>line_width_range</th>\n",
       "      <th>avg_line_offset</th>\n",
       "      <th>max_line_offset</th>\n",
       "      <th>min_line_offset</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>genre</th>\n",
       "      <th>sentence_count</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1878-10-26</td>\n",
       "      <td>KUMAT</td>\n",
       "      <td>Kumara Times</td>\n",
       "      <td>1</td>\n",
       "      <td>452.272727</td>\n",
       "      <td>282.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>230.0</td>\n",
       "      <td>33.090909</td>\n",
       "      <td>174.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>MAILS CLOSE</td>\n",
       "      <td>For the United Kingdom, Continent of Europe, a...</td>\n",
       "      <td>Notice</td>\n",
       "      <td>3</td>\n",
       "      <td>For the United Kingdom Continent of Europe and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1878-10-26</td>\n",
       "      <td>KUMAT</td>\n",
       "      <td>Kumara Times</td>\n",
       "      <td>3</td>\n",
       "      <td>429.500000</td>\n",
       "      <td>95.0</td>\n",
       "      <td>515.0</td>\n",
       "      <td>420.0</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>104.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>LATEST TELEGRAMS.</td>\n",
       "      <td>: [PRESS AGENCY.] i : ;; BtUPP, October 25. / ...</td>\n",
       "      <td>FamilyNotice</td>\n",
       "      <td>5</td>\n",
       "      <td>PRESS AGENCY i BtUPP October 25 Arrived Stella...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1878-10-26</td>\n",
       "      <td>KUMAT</td>\n",
       "      <td>Kumara Times</td>\n",
       "      <td>4</td>\n",
       "      <td>469.765854</td>\n",
       "      <td>64.0</td>\n",
       "      <td>589.0</td>\n",
       "      <td>525.0</td>\n",
       "      <td>22.575610</td>\n",
       "      <td>378.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>GENERAL ASSEMBLY.</td>\n",
       "      <td>Continued from 4th page. [press agency.] .;-•'...</td>\n",
       "      <td>Report</td>\n",
       "      <td>32</td>\n",
       "      <td>Continued from 4th page press agency - J LEGIS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1878-10-26</td>\n",
       "      <td>KUMAT</td>\n",
       "      <td>Kumara Times</td>\n",
       "      <td>5</td>\n",
       "      <td>480.044444</td>\n",
       "      <td>109.0</td>\n",
       "      <td>599.0</td>\n",
       "      <td>490.0</td>\n",
       "      <td>18.183333</td>\n",
       "      <td>244.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ADDITIONAL NEWS BY THE SAN FRANCISCO MAIL.</td>\n",
       "      <td>■ f &gt; • ; •* .■&lt;■ j.f&gt; '■' EUROPEAN ;/ ■ LoNi&gt;...</td>\n",
       "      <td>News</td>\n",
       "      <td>42</td>\n",
       "      <td>f j f EUROPEAN LoNi oii Septemb r 30i v - In C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1878-10-26</td>\n",
       "      <td>KUMAT</td>\n",
       "      <td>Kumara Times</td>\n",
       "      <td>6</td>\n",
       "      <td>461.769231</td>\n",
       "      <td>51.0</td>\n",
       "      <td>531.0</td>\n",
       "      <td>480.0</td>\n",
       "      <td>22.745562</td>\n",
       "      <td>363.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>GENERAL ASSEMBLY.</td>\n",
       "      <td>'pPKSSS AGENCY.] LEGISLATIVE COUNCIL. Wellingt...</td>\n",
       "      <td>Report</td>\n",
       "      <td>39</td>\n",
       "      <td>pPKSSS AGENCY LEGISLATIVE COUNCIL Wellington O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4613</th>\n",
       "      <td>1858-10-15</td>\n",
       "      <td>DSC</td>\n",
       "      <td>Daily Southern Cross</td>\n",
       "      <td>8</td>\n",
       "      <td>587.000000</td>\n",
       "      <td>508.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>158.0</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>BIRTH</td>\n",
       "      <td>At Wesley College. Auckland, on the 12th Octob...</td>\n",
       "      <td>FamilyNotice</td>\n",
       "      <td>3</td>\n",
       "      <td>At Wesley College Auckland on the 12th October...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4614</th>\n",
       "      <td>1858-10-15</td>\n",
       "      <td>DSC</td>\n",
       "      <td>Daily Southern Cross</td>\n",
       "      <td>9</td>\n",
       "      <td>536.000000</td>\n",
       "      <td>404.0</td>\n",
       "      <td>668.0</td>\n",
       "      <td>264.0</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>DIED.</td>\n",
       "      <td>On Monday, the 11th instant, Mary, second daug...</td>\n",
       "      <td>FamilyNotice</td>\n",
       "      <td>1</td>\n",
       "      <td>On Monday the 11th instant Mary second daughte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4615</th>\n",
       "      <td>1858-10-15</td>\n",
       "      <td>DSC</td>\n",
       "      <td>Daily Southern Cross</td>\n",
       "      <td>10</td>\n",
       "      <td>656.030075</td>\n",
       "      <td>18.0</td>\n",
       "      <td>704.0</td>\n",
       "      <td>686.0</td>\n",
       "      <td>35.338346</td>\n",
       "      <td>368.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>THE COUNCIL.</td>\n",
       "      <td>We have always maintained, that of the two bra...</td>\n",
       "      <td>Opinion</td>\n",
       "      <td>36</td>\n",
       "      <td>We have always maintained that of the two bran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4617</th>\n",
       "      <td>1858-10-15</td>\n",
       "      <td>DSC</td>\n",
       "      <td>Daily Southern Cross</td>\n",
       "      <td>12</td>\n",
       "      <td>612.764706</td>\n",
       "      <td>96.0</td>\n",
       "      <td>704.0</td>\n",
       "      <td>608.0</td>\n",
       "      <td>21.176471</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>PRESBYTERY OF AUCKLAND</td>\n",
       "      <td>, The principal meeting for the year of this C...</td>\n",
       "      <td>Report</td>\n",
       "      <td>22</td>\n",
       "      <td>The principal meeting for the year of this Cou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4621</th>\n",
       "      <td>1858-10-15</td>\n",
       "      <td>DSC</td>\n",
       "      <td>Daily Southern Cross</td>\n",
       "      <td>16</td>\n",
       "      <td>586.627451</td>\n",
       "      <td>39.0</td>\n",
       "      <td>672.0</td>\n",
       "      <td>633.0</td>\n",
       "      <td>29.450980</td>\n",
       "      <td>579.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Correspondence.</td>\n",
       "      <td>To the Members of the Southern Division. Gentl...</td>\n",
       "      <td>LetterToEditor</td>\n",
       "      <td>18</td>\n",
       "      <td>To the Members of the Southern Division Gentle...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3518 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           date newspaper_id             newspaper  article_id  \\\n",
       "0    1878-10-26        KUMAT          Kumara Times           1   \n",
       "2    1878-10-26        KUMAT          Kumara Times           3   \n",
       "3    1878-10-26        KUMAT          Kumara Times           4   \n",
       "4    1878-10-26        KUMAT          Kumara Times           5   \n",
       "5    1878-10-26        KUMAT          Kumara Times           6   \n",
       "...         ...          ...                   ...         ...   \n",
       "4613 1858-10-15          DSC  Daily Southern Cross           8   \n",
       "4614 1858-10-15          DSC  Daily Southern Cross           9   \n",
       "4615 1858-10-15          DSC  Daily Southern Cross          10   \n",
       "4617 1858-10-15          DSC  Daily Southern Cross          12   \n",
       "4621 1858-10-15          DSC  Daily Southern Cross          16   \n",
       "\n",
       "      avg_line_width  min_line_width  max_line_width  line_width_range  \\\n",
       "0         452.272727           282.0           512.0             230.0   \n",
       "2         429.500000            95.0           515.0             420.0   \n",
       "3         469.765854            64.0           589.0             525.0   \n",
       "4         480.044444           109.0           599.0             490.0   \n",
       "5         461.769231            51.0           531.0             480.0   \n",
       "...              ...             ...             ...               ...   \n",
       "4613      587.000000           508.0           666.0             158.0   \n",
       "4614      536.000000           404.0           668.0             264.0   \n",
       "4615      656.030075            18.0           704.0             686.0   \n",
       "4617      612.764706            96.0           704.0             608.0   \n",
       "4621      586.627451            39.0           672.0             633.0   \n",
       "\n",
       "      avg_line_offset  max_line_offset  min_line_offset  \\\n",
       "0           33.090909            174.0              0.0   \n",
       "2           22.000000            104.0              0.0   \n",
       "3           22.575610            378.0              0.0   \n",
       "4           18.183333            244.0              0.0   \n",
       "5           22.745562            363.0              0.0   \n",
       "...               ...              ...              ...   \n",
       "4613        12.000000             24.0              0.0   \n",
       "4614        12.500000             25.0              0.0   \n",
       "4615        35.338346            368.0              0.0   \n",
       "4617        21.176471             67.0              0.0   \n",
       "4621        29.450980            579.0              0.0   \n",
       "\n",
       "                                           title  \\\n",
       "0                                    MAILS CLOSE   \n",
       "2                              LATEST TELEGRAMS.   \n",
       "3                              GENERAL ASSEMBLY.   \n",
       "4     ADDITIONAL NEWS BY THE SAN FRANCISCO MAIL.   \n",
       "5                              GENERAL ASSEMBLY.   \n",
       "...                                          ...   \n",
       "4613                                       BIRTH   \n",
       "4614                                       DIED.   \n",
       "4615                                THE COUNCIL.   \n",
       "4617                      PRESBYTERY OF AUCKLAND   \n",
       "4621                             Correspondence.   \n",
       "\n",
       "                                                   text           genre  \\\n",
       "0     For the United Kingdom, Continent of Europe, a...          Notice   \n",
       "2     : [PRESS AGENCY.] i : ;; BtUPP, October 25. / ...    FamilyNotice   \n",
       "3     Continued from 4th page. [press agency.] .;-•'...          Report   \n",
       "4     ■ f > • ; •* .■<■ j.f> '■' EUROPEAN ;/ ■ LoNi>...            News   \n",
       "5     'pPKSSS AGENCY.] LEGISLATIVE COUNCIL. Wellingt...          Report   \n",
       "...                                                 ...             ...   \n",
       "4613  At Wesley College. Auckland, on the 12th Octob...    FamilyNotice   \n",
       "4614  On Monday, the 11th instant, Mary, second daug...    FamilyNotice   \n",
       "4615  We have always maintained, that of the two bra...         Opinion   \n",
       "4617  , The principal meeting for the year of this C...          Report   \n",
       "4621  To the Members of the Southern Division. Gentl...  LetterToEditor   \n",
       "\n",
       "      sentence_count                                         clean_text  \n",
       "0                  3  For the United Kingdom Continent of Europe and...  \n",
       "2                  5  PRESS AGENCY i BtUPP October 25 Arrived Stella...  \n",
       "3                 32  Continued from 4th page press agency - J LEGIS...  \n",
       "4                 42  f j f EUROPEAN LoNi oii Septemb r 30i v - In C...  \n",
       "5                 39  pPKSSS AGENCY LEGISLATIVE COUNCIL Wellington O...  \n",
       "...              ...                                                ...  \n",
       "4613               3  At Wesley College Auckland on the 12th October...  \n",
       "4614               1  On Monday the 11th instant Mary second daughte...  \n",
       "4615              36  We have always maintained that of the two bran...  \n",
       "4617              22  The principal meeting for the year of this Cou...  \n",
       "4621              18  To the Members of the Southern Division Gentle...  \n",
       "\n",
       "[3518 rows x 16 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "display(clean_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions\n",
    "\n",
    "Run the following cells to define the functions that will be used to extract features from the text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_propn_spacy(doc):\n",
    "    \"\"\"\n",
    "    Given a Spacy doc object return counts of the \n",
    "    following parts-of-speech: proper nouns.\n",
    "    \n",
    "    Optional: uncomment the code lines to collect and \n",
    "    print a list of the tagged words.\n",
    "    \"\"\"\n",
    "    count_propn = 0\n",
    "    # propn_list = []\n",
    "\n",
    "    for token in doc:\n",
    "        if token.pos_ == 'PROPN':\n",
    "            count_propn += 1\n",
    "        \n",
    "    return count_propn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_verb_spacy(doc):\n",
    "    \"\"\"\n",
    "    Given a Spacy doc object return counts of the \n",
    "    following parts-of-speech: verbs.\n",
    "    \n",
    "    Optional: uncomment the code lines to collect and \n",
    "    print a list of the tagged words.\n",
    "    \"\"\"\n",
    "    count_verb = 0\n",
    "    # verb_list = []\n",
    "\n",
    "    for token in doc:\n",
    "        if token.pos_ == 'VERB':\n",
    "            count_verb += 1\n",
    "\n",
    "            # verb_list.append(token)\n",
    "    # print(verb_list)\n",
    "\n",
    "    return count_verb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_noun_spacy(doc):\n",
    "    \"\"\"\n",
    "    Given a Spacy doc object return counts of the \n",
    "    following parts-of-speech: nouns.\n",
    "    \n",
    "    Optional: uncomment the code lines to collect and \n",
    "    print a list of the tagged words.\n",
    "    \"\"\"\n",
    "    count_noun = 0\n",
    "    # noun_list = []\n",
    "\n",
    "    for token in doc:\n",
    "        if token.pos_ == 'NOUN':\n",
    "            count_noun += 1\n",
    "\n",
    "            # noun_list.append(token)\n",
    "    # print(noun_list)\n",
    "        \n",
    "    return count_noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_adj_spacy(doc):\n",
    "    \"\"\"\n",
    "    Given a Spacy doc object return counts of the \n",
    "    following parts-of-speech: adjectives.\n",
    "    \n",
    "    Optional: uncomment the code lines to collect and \n",
    "    print a list of the tagged words.\n",
    "    \"\"\"\n",
    "    count_adj = 0\n",
    "    # adj_list = []\n",
    "\n",
    "    for token in doc:\n",
    "        if token.pos_ == 'ADJ':\n",
    "            count_adj += 1\n",
    "\n",
    "            # adj_list.append(token)\n",
    "    # print(adj_list)\n",
    "        \n",
    "    return count_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_nums_spacy(doc):\n",
    "    \"\"\"\n",
    "    Given a Spacy doc object return counts of the \n",
    "    following parts-of-speech: numbers.\n",
    "    \n",
    "    Optional: uncomment the code lines to collect and \n",
    "    print a list of the tagged words.\n",
    "    \"\"\" \n",
    "    count_nums = 0\n",
    "    # nums_list = []\n",
    "\n",
    "    for token in doc:\n",
    "        if token.pos_ == 'NUM':\n",
    "            count_nums += 1\n",
    "\n",
    "            # nums_list.append(token)\n",
    "    # print(nums_list)\n",
    "        \n",
    "    return count_nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_pron_spacy(doc):\n",
    "    \"\"\"\n",
    "    Given a Spacy doc object return counts of the \n",
    "    following parts-of-speech: pronouns.\n",
    "    \n",
    "    Optional: uncomment the code lines to collect and \n",
    "    print a list of the tagged words.\n",
    "    \"\"\"\n",
    "    count_pron = 0\n",
    "    # pron_list = []\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.pos_ == 'PRON':\n",
    "            count_pron += 1\n",
    "\n",
    "            # pron_list.append(token)\n",
    "    # print(pron_list)\n",
    "        \n",
    "    return count_pron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_nnps_spacy(doc):\n",
    "    \"\"\"\n",
    "    Given a Spacy doc object return counts of the \n",
    "    following parts-of-speech: plural proper nouns.\n",
    "    \n",
    "    Optional: uncomment the code lines to collect and \n",
    "    print a list of the tagged words.\n",
    "    \"\"\"\n",
    "    count_nnps = 0\n",
    "    # nnps_list = []\n",
    "\n",
    "    for token in doc:\n",
    "        if token.tag_ == 'NNPS':\n",
    "            count_nnps += 1\n",
    "        \n",
    "    return count_nnps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_vb_spacy(doc):\n",
    "    \"\"\"\n",
    "    Given a Spacy doc object return counts of the \n",
    "    following parts-of-speech: base form verbs.\n",
    "    \n",
    "    Optional: uncomment the code lines to collect and \n",
    "    print a list of the tagged words.\n",
    "    \"\"\"\n",
    "    count_vb = 0\n",
    "    # vb_list = []\n",
    "\n",
    "    for token in doc:\n",
    "        if token.tag_ == 'VB':\n",
    "            count_vb += 1\n",
    "\n",
    "            # vb_list.append(token)\n",
    "    # print(vb_list)\n",
    "\n",
    "    return count_vb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_nn_spacy(doc):\n",
    "    \"\"\"\n",
    "    Given a Spacy doc object return counts of the \n",
    "    following parts-of-speech: singular or mass nouns.\n",
    "    \n",
    "    Optional: uncomment the code lines to collect and \n",
    "    print a list of the tagged words.\n",
    "    \"\"\"\n",
    "    count_nn = 0\n",
    "    # nn_list = []\n",
    "\n",
    "    for token in doc:\n",
    "        if token.tag_ == 'NN':\n",
    "            count_nn += 1\n",
    "\n",
    "            # nn_list.append(token)\n",
    "    # print(nn_list)\n",
    "        \n",
    "    return count_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_jj_spacy(doc):\n",
    "    \"\"\"\n",
    "    Given a Spacy doc object return counts of the \n",
    "    following parts-of-speech: adjectives.\n",
    "    \n",
    "    Optional: uncomment the code lines to collect and \n",
    "    print a list of the tagged words.\n",
    "    \"\"\"\n",
    "    count_jj = 0\n",
    "    # jj_list = []\n",
    "\n",
    "    for token in doc:\n",
    "        if token.tag_ == 'JJ':\n",
    "            count_jj += 1\n",
    "\n",
    "            # jj_list.append(token)\n",
    "    # print(jj_list)\n",
    "        \n",
    "    return count_jj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_cd_spacy(doc):\n",
    "    \"\"\"\n",
    "    Given a Spacy doc object return counts of the \n",
    "    following parts-of-speech: cardinal numbers.\n",
    "    \n",
    "    Optional: uncomment the code lines to collect and \n",
    "    print a list of the tagged words.\n",
    "    \"\"\" \n",
    "    count_cd = 0\n",
    "    # cd_list = []\n",
    "\n",
    "    for token in doc:\n",
    "        if token.tag_ == 'CD':\n",
    "            count_cd += 1\n",
    "\n",
    "            # cd_list.append(token)\n",
    "    # print(cd_list)\n",
    "        \n",
    "    return count_cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_prp_spacy(doc):\n",
    "    \"\"\"\n",
    "    Given a Spacy doc object return counts of the \n",
    "    following parts-of-speech: personal pronouns.\n",
    "    \n",
    "    Optional: uncomment the code lines to collect and \n",
    "    print a list of the tagged words.\n",
    "    \"\"\"\n",
    "    count_prp = 0\n",
    "    # prp_list = []\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.tag_ == 'PRP':\n",
    "            count_prp += 1\n",
    "\n",
    "            # prp_list.append(token)\n",
    "    # print(prp_list)\n",
    "        \n",
    "    return count_prp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_rb_spacy(doc):\n",
    "    \"\"\"\n",
    "    Given a Spacy doc object return counts of the \n",
    "    following parts-of-speech: adverbs.\n",
    "    \n",
    "    Optional: uncomment the code lines to collect and \n",
    "    print a list of the tagged words.\n",
    "    \"\"\"\n",
    "    count_rb = 0\n",
    "    # rb_list = []\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.tag_ == 'RB':\n",
    "            count_rb += 1\n",
    "\n",
    "            # rb_list.append(token)\n",
    "    # print(rb_list)\n",
    "        \n",
    "    return count_rb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_cc_spacy(doc):\n",
    "    \"\"\"\n",
    "    Given a Spacy doc object return counts of the \n",
    "    following parts-of-speech: coordinating conjunctions.\n",
    "    \n",
    "    Optional: uncomment the code lines to collect and \n",
    "    print a list of the tagged words.\n",
    "    \"\"\"\n",
    "    count_cc = 0\n",
    "    # cc_list = []\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.tag_ == 'CC':\n",
    "            count_cc += 1\n",
    "\n",
    "            # cc_list.append(token)\n",
    "    # print(cc_list)\n",
    "        \n",
    "    return count_cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_nnp_spacy(doc):\n",
    "    \"\"\"\n",
    "    Given a Spacy doc object return counts of the \n",
    "    following parts-of-speech: singular proper nouns.\n",
    "    \n",
    "    Optional: uncomment the code lines to collect and \n",
    "    print a list of the tagged words.\n",
    "    \"\"\"\n",
    "    count_nnp = 0\n",
    "    # nnp_list = []\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.tag_ == 'NNP':\n",
    "            count_nnp += 1\n",
    "\n",
    "            # nnp_list.append(token)\n",
    "    # print(nnp_list)\n",
    "        \n",
    "    return count_nnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_vbd_spacy(doc):\n",
    "    \"\"\"\n",
    "    Given a Spacy doc object return counts of the \n",
    "    following parts-of-speech: past tense verbs.\n",
    "    \n",
    "    Optional: uncomment the code lines to collect and \n",
    "    print a list of the tagged words.\n",
    "    \"\"\"\n",
    "    count_vbd = 0\n",
    "    # vbd_list = []\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.tag_ == 'VBD':\n",
    "            count_vbd += 1\n",
    "\n",
    "            # vbd_list.append(token)\n",
    "    # print(vbd_list)\n",
    "        \n",
    "    return count_vbd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_vbz_spacy(doc):\n",
    "    \"\"\"\n",
    "    Given a Spacy doc object return counts of the \n",
    "    following parts-of-speech: third-person singular present verbs.\n",
    "    \n",
    "    Optional: uncomment the code lines to collect and \n",
    "    print a list of the tagged words.\n",
    "    \"\"\"\n",
    "    count_vbz = 0\n",
    "    # vbz_list = []\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.tag_ == 'VBZ':\n",
    "            count_vbz += 1\n",
    "\n",
    "            # vbz_list.append(token)\n",
    "    # print(vbz_list)\n",
    "        \n",
    "    return count_vbz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_features_pipe(text_col, df):\n",
    "    \"\"\"\n",
    "    Process given text column of a dataframe to \n",
    "    extract linguistic features and add them to\n",
    "    the dataframe. Return the updated dataframe.\n",
    "    \"\"\"\n",
    "    \n",
    "    input_col = df[text_col]  \n",
    "    \n",
    "    propn_count = []\n",
    "    verb_count = []\n",
    "    noun_count = []\n",
    "    adj_count = []\n",
    "    nums_count = []\n",
    "    pron_count = []\n",
    "    \n",
    "    nnps_count = []\n",
    "    vb_count = []\n",
    "    nn_count = []\n",
    "    jj_count = []\n",
    "    cd_count = []\n",
    "    prp_count = []\n",
    "    rb_count = []\n",
    "    cc_count = []\n",
    "    nnp_count = []\n",
    "    vbd_count = []\n",
    "    vbz_count = []\n",
    "    \n",
    "    # spaCy processing pipeline\n",
    "    nlp_text_pipe = nlp.pipe(input_col, batch_size=20)\n",
    "    \n",
    "    for doc in nlp_text_pipe:\n",
    "        \n",
    "        # POS tags\n",
    "        # Universal POS Tags\n",
    "        # http://universaldependencies.org/u/pos/\n",
    "        \n",
    "        # Count proper nouns\n",
    "        propn_total = 0\n",
    "        count_propn = count_propn_spacy(doc)\n",
    "        propn_total += count_propn\n",
    "        propn_count.append(propn_total)\n",
    "        \n",
    "        # Count verbs\n",
    "        verb_total = 0\n",
    "        count_verb = count_verb_spacy(doc)\n",
    "        verb_total += count_verb\n",
    "        verb_count.append(verb_total)\n",
    "        \n",
    "        # Count nouns\n",
    "        noun_total = 0\n",
    "        count_noun = count_noun_spacy(doc)\n",
    "        noun_total += count_noun\n",
    "        noun_count.append(noun_total)\n",
    "        \n",
    "        # Count adjectives\n",
    "        adj_total = 0\n",
    "        count_adj = count_adj_spacy(doc)\n",
    "        adj_total += count_adj\n",
    "        adj_count.append(adj_total)\n",
    "        \n",
    "        # Count numbers\n",
    "        nums_total = 0\n",
    "        count_nums = count_nums_spacy(doc)\n",
    "        nums_total += count_nums\n",
    "        nums_count.append(nums_total)\n",
    "        \n",
    "        # Count pronouns\n",
    "        pron_total = 0\n",
    "        count_pron = count_pron_spacy(doc)\n",
    "        pron_total += count_pron\n",
    "        pron_count.append(pron_total)\n",
    "        \n",
    "        # POS tags (English)\n",
    "        # OntoNotes 5 / Penn Treebank\n",
    "        # https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "        \n",
    "        # Count plural proper nouns\n",
    "        nnps_total = 0\n",
    "        count_nnps = count_nnps_spacy(doc)\n",
    "        nnps_total += count_nnps\n",
    "        nnps_count.append(nnps_total)\n",
    "        \n",
    "        # Count base form verbs\n",
    "        vb_total = 0\n",
    "        count_vb = count_vb_spacy(doc)\n",
    "        vb_total += count_vb\n",
    "        vb_count.append(vb_total)\n",
    "        \n",
    "        # Count singular or mass nouns\n",
    "        nn_total = 0\n",
    "        count_nn = count_nn_spacy(doc)\n",
    "        nn_total += count_nn\n",
    "        nn_count.append(nn_total)\n",
    "        \n",
    "        # Count adjectives\n",
    "        jj_total = 0\n",
    "        count_jj = count_jj_spacy(doc)\n",
    "        jj_total += count_jj\n",
    "        jj_count.append(jj_total)\n",
    "        \n",
    "        # Count cardinal numbers\n",
    "        cd_total = 0\n",
    "        count_cd = count_cd_spacy(doc)\n",
    "        cd_total += count_cd\n",
    "        cd_count.append(cd_total)\n",
    "        \n",
    "        # Count personal pronouns\n",
    "        prp_total = 0\n",
    "        count_prp = count_prp_spacy(doc)\n",
    "        prp_total += count_prp\n",
    "        prp_count.append(prp_total)\n",
    "        \n",
    "        # Count adverbs\n",
    "        rb_total = 0\n",
    "        count_rb = count_rb_spacy(doc)\n",
    "        rb_total += count_rb\n",
    "        rb_count.append(rb_total)\n",
    "        \n",
    "        # Count coordinating conjunctions\n",
    "        cc_total = 0\n",
    "        count_cc = count_cc_spacy(doc)\n",
    "        cc_total += count_cc\n",
    "        cc_count.append(cc_total)\n",
    "        \n",
    "        # Count singular proper nouns\n",
    "        nnp_total = 0\n",
    "        count_nnp = count_nnp_spacy(doc)\n",
    "        nnp_total += count_nnp\n",
    "        nnp_count.append(nnp_total)\n",
    "        \n",
    "        # Count past tense verbs\n",
    "        vbd_total = 0\n",
    "        count_vbd = count_vbd_spacy(doc)\n",
    "        vbd_total += count_vbd\n",
    "        vbd_count.append(vbd_total)\n",
    "        \n",
    "        # Count third-person singular present verbs\n",
    "        vbz_total = 0\n",
    "        count_vbz = count_vbz_spacy(doc)\n",
    "        vbz_total += count_vbz\n",
    "        vbz_count.append(vbz_total)\n",
    "        \n",
    "    # Add features using the textstat library to the dataframe\n",
    "    # https://pypi.org/project/textstat/\n",
    "    df['word_count'] = input_col.apply(lambda x: textstat.lexicon_count(x, removepunct=True)) \n",
    "    df['syll_count'] = input_col.apply(lambda x: textstat.syllable_count(x))\n",
    "    df['polysyll_count'] = input_col.apply(lambda x: textstat.polysyllabcount(x)) # Returns the number of words with a syllable count greater than or equal to 3.\n",
    "    df['monosyll_count'] = input_col.apply(lambda x: textstat.monosyllabcount(x)) # Returns the number of words with a syllable count equal to one.\n",
    "    \n",
    "    # Add features using the textfeatures library to the dataframe\n",
    "    # https://towardsdatascience.com/textfeatures-library-for-extracting-basic-features-from-text-data-f98ba90e3932\n",
    "    tf.stopwords_count(df,text_col,'stopwords_count')\n",
    "    # tf.stopwords(df,text_col,'stopwords')  # Include a column that lists the stopwords found in the text\n",
    "\n",
    "    try:\n",
    "        tf.avg_word_length(df,text_col,'avg_word_length')\n",
    "    except:\n",
    "        df['avg_word_length'] = 0\n",
    "    \n",
    "    try:\n",
    "        tf.char_count(df,text_col,'char_count')\n",
    "    except:\n",
    "        df['char_count'] = 0\n",
    "    \n",
    "    # Add features based on the spaCy pipeline to the dataframe\n",
    "    df['propn_count'] = propn_count\n",
    "    df['verb_count'] = verb_count\n",
    "    df['noun_count'] = noun_count\n",
    "    df['adj_count'] = adj_count\n",
    "    df['nums_count'] = nums_count\n",
    "    df['pron_count'] = pron_count\n",
    "    \n",
    "    df['nnps_count'] = nnps_count\n",
    "    df['vb_count'] = vb_count\n",
    "    df['nn_count'] = nn_count\n",
    "    df['jj_count'] = jj_count\n",
    "    df['cd_count'] = cd_count\n",
    "    df['prp_count'] = prp_count\n",
    "    df['rb_count'] = rb_count\n",
    "    df['cc_count'] = cc_count\n",
    "    df['nnp_count'] = nnp_count\n",
    "    df['vbd_count'] = vbd_count\n",
    "    df['vbz_count'] = vbz_count\n",
    "    \n",
    "    # Add frequency columns\n",
    "    \n",
    "    df['propn_freq'] = df['propn_count']/df['word_count']\n",
    "    df['verb_freq'] = df['verb_count']/df['word_count']\n",
    "    df['noun_freq'] = df['noun_count']/df['word_count']\n",
    "    df['adj_freq'] = df['adj_count']/df['word_count']\n",
    "    df['nums_freq'] = df['nums_count']/df['word_count']\n",
    "    df['pron_freq'] = df['pron_count']/df['word_count']\n",
    "    \n",
    "    df['nnps_freq'] = df['nnps_count']/df['word_count']\n",
    "    df['vb_freq'] = df['vb_count']/df['word_count']\n",
    "    df['nn_freq'] = df['nn_count']/df['word_count']\n",
    "    df['jj_freq'] = df['jj_count']/df['word_count']\n",
    "    df['cd_freq'] = df['cd_count']/df['word_count']\n",
    "    df['prp_freq'] = df['prp_count']/df['word_count']\n",
    "    df['rb_freq'] = df['rb_count']/df['word_count']\n",
    "    df['cc_freq'] = df['cc_count']/df['word_count']\n",
    "    df['nnp_freq'] = df['nnp_count']/df['word_count']\n",
    "    df['vbd_freq'] = df['vbd_count']/df['word_count']\n",
    "    df['vbz_freq'] = df['vbz_count']/df['word_count']\n",
    "    \n",
    "    df['polysyll_freq'] = df['polysyll_count']/df['word_count']\n",
    "    df['monosyll_freq'] = df['monosyll_count']/df['word_count']\n",
    "    df['stopword_freq'] = df['stopwords_count']/df['word_count']\n",
    "    \n",
    "    return df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the text feature extraction pipeline\n",
    "\n",
    "* Provide the dataframe and the name of the text column to extract features from.\n",
    "* Return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_col = 'clean_text'  # The name of dataframe column containing the text to be processed\n",
    "features_df = text_features_pipe(text_col, clean_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df.reset_index(drop=True, inplace=True) # reset the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop any rows that have NaN values (the avg_line_width column is used - which will be NaN for any empty articles)\n",
    "features_df.dropna(subset = [\"avg_line_width\"], inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>newspaper_id</th>\n",
       "      <th>newspaper</th>\n",
       "      <th>article_id</th>\n",
       "      <th>avg_line_width</th>\n",
       "      <th>min_line_width</th>\n",
       "      <th>max_line_width</th>\n",
       "      <th>line_width_range</th>\n",
       "      <th>avg_line_offset</th>\n",
       "      <th>max_line_offset</th>\n",
       "      <th>min_line_offset</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>genre</th>\n",
       "      <th>sentence_count</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>word_count</th>\n",
       "      <th>syll_count</th>\n",
       "      <th>polysyll_count</th>\n",
       "      <th>monosyll_count</th>\n",
       "      <th>stopwords_count</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>char_count</th>\n",
       "      <th>propn_count</th>\n",
       "      <th>verb_count</th>\n",
       "      <th>noun_count</th>\n",
       "      <th>adj_count</th>\n",
       "      <th>nums_count</th>\n",
       "      <th>pron_count</th>\n",
       "      <th>nnps_count</th>\n",
       "      <th>vb_count</th>\n",
       "      <th>nn_count</th>\n",
       "      <th>jj_count</th>\n",
       "      <th>cd_count</th>\n",
       "      <th>prp_count</th>\n",
       "      <th>rb_count</th>\n",
       "      <th>cc_count</th>\n",
       "      <th>nnp_count</th>\n",
       "      <th>vbd_count</th>\n",
       "      <th>vbz_count</th>\n",
       "      <th>propn_freq</th>\n",
       "      <th>verb_freq</th>\n",
       "      <th>noun_freq</th>\n",
       "      <th>adj_freq</th>\n",
       "      <th>nums_freq</th>\n",
       "      <th>pron_freq</th>\n",
       "      <th>nnps_freq</th>\n",
       "      <th>vb_freq</th>\n",
       "      <th>nn_freq</th>\n",
       "      <th>jj_freq</th>\n",
       "      <th>cd_freq</th>\n",
       "      <th>prp_freq</th>\n",
       "      <th>rb_freq</th>\n",
       "      <th>cc_freq</th>\n",
       "      <th>nnp_freq</th>\n",
       "      <th>vbd_freq</th>\n",
       "      <th>vbz_freq</th>\n",
       "      <th>polysyll_freq</th>\n",
       "      <th>monosyll_freq</th>\n",
       "      <th>stopword_freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1878-10-26</td>\n",
       "      <td>KUMAT</td>\n",
       "      <td>Kumara Times</td>\n",
       "      <td>1</td>\n",
       "      <td>452.272727</td>\n",
       "      <td>282.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>230.0</td>\n",
       "      <td>33.090909</td>\n",
       "      <td>174.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>MAILS CLOSE</td>\n",
       "      <td>For the United Kingdom, Continent of Europe, a...</td>\n",
       "      <td>Notice</td>\n",
       "      <td>3</td>\n",
       "      <td>For the United Kingdom Continent of Europe and...</td>\n",
       "      <td>57</td>\n",
       "      <td>87</td>\n",
       "      <td>7</td>\n",
       "      <td>35</td>\n",
       "      <td>15</td>\n",
       "      <td>4.620690</td>\n",
       "      <td>325</td>\n",
       "      <td>25</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.438596</td>\n",
       "      <td>0.070175</td>\n",
       "      <td>0.122807</td>\n",
       "      <td>0.017544</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035088</td>\n",
       "      <td>0.035088</td>\n",
       "      <td>0.087719</td>\n",
       "      <td>0.017544</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017544</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.403509</td>\n",
       "      <td>0.017544</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.122807</td>\n",
       "      <td>0.614035</td>\n",
       "      <td>0.263158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1878-10-26</td>\n",
       "      <td>KUMAT</td>\n",
       "      <td>Kumara Times</td>\n",
       "      <td>3</td>\n",
       "      <td>429.500000</td>\n",
       "      <td>95.0</td>\n",
       "      <td>515.0</td>\n",
       "      <td>420.0</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>104.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>LATEST TELEGRAMS.</td>\n",
       "      <td>: [PRESS AGENCY.] i : ;; BtUPP, October 25. / ...</td>\n",
       "      <td>FamilyNotice</td>\n",
       "      <td>5</td>\n",
       "      <td>PRESS AGENCY i BtUPP October 25 Arrived Stella...</td>\n",
       "      <td>46</td>\n",
       "      <td>72</td>\n",
       "      <td>5</td>\n",
       "      <td>30</td>\n",
       "      <td>10</td>\n",
       "      <td>4.098039</td>\n",
       "      <td>259</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.347826</td>\n",
       "      <td>0.065217</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.347826</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.108696</td>\n",
       "      <td>0.652174</td>\n",
       "      <td>0.217391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1878-10-26</td>\n",
       "      <td>KUMAT</td>\n",
       "      <td>Kumara Times</td>\n",
       "      <td>4</td>\n",
       "      <td>469.765854</td>\n",
       "      <td>64.0</td>\n",
       "      <td>589.0</td>\n",
       "      <td>525.0</td>\n",
       "      <td>22.575610</td>\n",
       "      <td>378.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>GENERAL ASSEMBLY.</td>\n",
       "      <td>Continued from 4th page. [press agency.] .;-•'...</td>\n",
       "      <td>Report</td>\n",
       "      <td>32</td>\n",
       "      <td>Continued from 4th page press agency - J LEGIS...</td>\n",
       "      <td>1131</td>\n",
       "      <td>1628</td>\n",
       "      <td>104</td>\n",
       "      <td>794</td>\n",
       "      <td>337</td>\n",
       "      <td>4.469317</td>\n",
       "      <td>6327</td>\n",
       "      <td>298</td>\n",
       "      <td>123</td>\n",
       "      <td>235</td>\n",
       "      <td>40</td>\n",
       "      <td>36</td>\n",
       "      <td>27</td>\n",
       "      <td>7</td>\n",
       "      <td>44</td>\n",
       "      <td>185</td>\n",
       "      <td>35</td>\n",
       "      <td>36</td>\n",
       "      <td>22</td>\n",
       "      <td>16</td>\n",
       "      <td>22</td>\n",
       "      <td>292</td>\n",
       "      <td>51</td>\n",
       "      <td>5</td>\n",
       "      <td>0.263484</td>\n",
       "      <td>0.108753</td>\n",
       "      <td>0.207781</td>\n",
       "      <td>0.035367</td>\n",
       "      <td>0.031830</td>\n",
       "      <td>0.023873</td>\n",
       "      <td>0.006189</td>\n",
       "      <td>0.038904</td>\n",
       "      <td>0.163572</td>\n",
       "      <td>0.030946</td>\n",
       "      <td>0.031830</td>\n",
       "      <td>0.019452</td>\n",
       "      <td>0.014147</td>\n",
       "      <td>0.019452</td>\n",
       "      <td>0.258179</td>\n",
       "      <td>0.045093</td>\n",
       "      <td>0.004421</td>\n",
       "      <td>0.091954</td>\n",
       "      <td>0.702034</td>\n",
       "      <td>0.297966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1878-10-26</td>\n",
       "      <td>KUMAT</td>\n",
       "      <td>Kumara Times</td>\n",
       "      <td>5</td>\n",
       "      <td>480.044444</td>\n",
       "      <td>109.0</td>\n",
       "      <td>599.0</td>\n",
       "      <td>490.0</td>\n",
       "      <td>18.183333</td>\n",
       "      <td>244.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ADDITIONAL NEWS BY THE SAN FRANCISCO MAIL.</td>\n",
       "      <td>■ f &gt; • ; •* .■&lt;■ j.f&gt; '■' EUROPEAN ;/ ■ LoNi&gt;...</td>\n",
       "      <td>News</td>\n",
       "      <td>42</td>\n",
       "      <td>f j f EUROPEAN LoNi oii Septemb r 30i v - In C...</td>\n",
       "      <td>1079</td>\n",
       "      <td>1576</td>\n",
       "      <td>103</td>\n",
       "      <td>744</td>\n",
       "      <td>370</td>\n",
       "      <td>4.538462</td>\n",
       "      <td>6119</td>\n",
       "      <td>255</td>\n",
       "      <td>111</td>\n",
       "      <td>219</td>\n",
       "      <td>56</td>\n",
       "      <td>29</td>\n",
       "      <td>40</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>174</td>\n",
       "      <td>55</td>\n",
       "      <td>29</td>\n",
       "      <td>21</td>\n",
       "      <td>14</td>\n",
       "      <td>37</td>\n",
       "      <td>247</td>\n",
       "      <td>46</td>\n",
       "      <td>27</td>\n",
       "      <td>0.236330</td>\n",
       "      <td>0.102873</td>\n",
       "      <td>0.202966</td>\n",
       "      <td>0.051900</td>\n",
       "      <td>0.026877</td>\n",
       "      <td>0.037071</td>\n",
       "      <td>0.007414</td>\n",
       "      <td>0.011121</td>\n",
       "      <td>0.161260</td>\n",
       "      <td>0.050973</td>\n",
       "      <td>0.026877</td>\n",
       "      <td>0.019462</td>\n",
       "      <td>0.012975</td>\n",
       "      <td>0.034291</td>\n",
       "      <td>0.228916</td>\n",
       "      <td>0.042632</td>\n",
       "      <td>0.025023</td>\n",
       "      <td>0.095459</td>\n",
       "      <td>0.689527</td>\n",
       "      <td>0.342910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1878-10-26</td>\n",
       "      <td>KUMAT</td>\n",
       "      <td>Kumara Times</td>\n",
       "      <td>6</td>\n",
       "      <td>461.769231</td>\n",
       "      <td>51.0</td>\n",
       "      <td>531.0</td>\n",
       "      <td>480.0</td>\n",
       "      <td>22.745562</td>\n",
       "      <td>363.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>GENERAL ASSEMBLY.</td>\n",
       "      <td>'pPKSSS AGENCY.] LEGISLATIVE COUNCIL. Wellingt...</td>\n",
       "      <td>Report</td>\n",
       "      <td>39</td>\n",
       "      <td>pPKSSS AGENCY LEGISLATIVE COUNCIL Wellington O...</td>\n",
       "      <td>1043</td>\n",
       "      <td>1518</td>\n",
       "      <td>115</td>\n",
       "      <td>711</td>\n",
       "      <td>373</td>\n",
       "      <td>4.614573</td>\n",
       "      <td>5855</td>\n",
       "      <td>254</td>\n",
       "      <td>121</td>\n",
       "      <td>175</td>\n",
       "      <td>37</td>\n",
       "      <td>25</td>\n",
       "      <td>35</td>\n",
       "      <td>9</td>\n",
       "      <td>39</td>\n",
       "      <td>132</td>\n",
       "      <td>37</td>\n",
       "      <td>25</td>\n",
       "      <td>26</td>\n",
       "      <td>17</td>\n",
       "      <td>23</td>\n",
       "      <td>245</td>\n",
       "      <td>55</td>\n",
       "      <td>2</td>\n",
       "      <td>0.243528</td>\n",
       "      <td>0.116012</td>\n",
       "      <td>0.167785</td>\n",
       "      <td>0.035475</td>\n",
       "      <td>0.023969</td>\n",
       "      <td>0.033557</td>\n",
       "      <td>0.008629</td>\n",
       "      <td>0.037392</td>\n",
       "      <td>0.126558</td>\n",
       "      <td>0.035475</td>\n",
       "      <td>0.023969</td>\n",
       "      <td>0.024928</td>\n",
       "      <td>0.016299</td>\n",
       "      <td>0.022052</td>\n",
       "      <td>0.234899</td>\n",
       "      <td>0.052733</td>\n",
       "      <td>0.001918</td>\n",
       "      <td>0.110259</td>\n",
       "      <td>0.681687</td>\n",
       "      <td>0.357622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3513</th>\n",
       "      <td>1858-10-15</td>\n",
       "      <td>DSC</td>\n",
       "      <td>Daily Southern Cross</td>\n",
       "      <td>8</td>\n",
       "      <td>587.000000</td>\n",
       "      <td>508.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>158.0</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>BIRTH</td>\n",
       "      <td>At Wesley College. Auckland, on the 12th Octob...</td>\n",
       "      <td>FamilyNotice</td>\n",
       "      <td>3</td>\n",
       "      <td>At Wesley College Auckland on the 12th October...</td>\n",
       "      <td>18</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>3.944444</td>\n",
       "      <td>88</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3514</th>\n",
       "      <td>1858-10-15</td>\n",
       "      <td>DSC</td>\n",
       "      <td>Daily Southern Cross</td>\n",
       "      <td>9</td>\n",
       "      <td>536.000000</td>\n",
       "      <td>404.0</td>\n",
       "      <td>668.0</td>\n",
       "      <td>264.0</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>DIED.</td>\n",
       "      <td>On Monday, the 11th instant, Mary, second daug...</td>\n",
       "      <td>FamilyNotice</td>\n",
       "      <td>1</td>\n",
       "      <td>On Monday the 11th instant Mary second daughte...</td>\n",
       "      <td>14</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>4.571429</td>\n",
       "      <td>77</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3515</th>\n",
       "      <td>1858-10-15</td>\n",
       "      <td>DSC</td>\n",
       "      <td>Daily Southern Cross</td>\n",
       "      <td>10</td>\n",
       "      <td>656.030075</td>\n",
       "      <td>18.0</td>\n",
       "      <td>704.0</td>\n",
       "      <td>686.0</td>\n",
       "      <td>35.338346</td>\n",
       "      <td>368.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>THE COUNCIL.</td>\n",
       "      <td>We have always maintained, that of the two bra...</td>\n",
       "      <td>Opinion</td>\n",
       "      <td>36</td>\n",
       "      <td>We have always maintained that of the two bran...</td>\n",
       "      <td>1088</td>\n",
       "      <td>1580</td>\n",
       "      <td>103</td>\n",
       "      <td>750</td>\n",
       "      <td>536</td>\n",
       "      <td>4.573921</td>\n",
       "      <td>6069</td>\n",
       "      <td>69</td>\n",
       "      <td>123</td>\n",
       "      <td>203</td>\n",
       "      <td>77</td>\n",
       "      <td>6</td>\n",
       "      <td>97</td>\n",
       "      <td>4</td>\n",
       "      <td>54</td>\n",
       "      <td>163</td>\n",
       "      <td>70</td>\n",
       "      <td>6</td>\n",
       "      <td>53</td>\n",
       "      <td>55</td>\n",
       "      <td>38</td>\n",
       "      <td>65</td>\n",
       "      <td>27</td>\n",
       "      <td>33</td>\n",
       "      <td>0.063419</td>\n",
       "      <td>0.113051</td>\n",
       "      <td>0.186581</td>\n",
       "      <td>0.070772</td>\n",
       "      <td>0.005515</td>\n",
       "      <td>0.089154</td>\n",
       "      <td>0.003676</td>\n",
       "      <td>0.049632</td>\n",
       "      <td>0.149816</td>\n",
       "      <td>0.064338</td>\n",
       "      <td>0.005515</td>\n",
       "      <td>0.048713</td>\n",
       "      <td>0.050551</td>\n",
       "      <td>0.034926</td>\n",
       "      <td>0.059743</td>\n",
       "      <td>0.024816</td>\n",
       "      <td>0.030331</td>\n",
       "      <td>0.094669</td>\n",
       "      <td>0.689338</td>\n",
       "      <td>0.492647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3516</th>\n",
       "      <td>1858-10-15</td>\n",
       "      <td>DSC</td>\n",
       "      <td>Daily Southern Cross</td>\n",
       "      <td>12</td>\n",
       "      <td>612.764706</td>\n",
       "      <td>96.0</td>\n",
       "      <td>704.0</td>\n",
       "      <td>608.0</td>\n",
       "      <td>21.176471</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>PRESBYTERY OF AUCKLAND</td>\n",
       "      <td>, The principal meeting for the year of this C...</td>\n",
       "      <td>Report</td>\n",
       "      <td>22</td>\n",
       "      <td>The principal meeting for the year of this Cou...</td>\n",
       "      <td>575</td>\n",
       "      <td>845</td>\n",
       "      <td>74</td>\n",
       "      <td>400</td>\n",
       "      <td>267</td>\n",
       "      <td>4.680556</td>\n",
       "      <td>3271</td>\n",
       "      <td>91</td>\n",
       "      <td>64</td>\n",
       "      <td>92</td>\n",
       "      <td>26</td>\n",
       "      <td>7</td>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>66</td>\n",
       "      <td>25</td>\n",
       "      <td>7</td>\n",
       "      <td>15</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>87</td>\n",
       "      <td>29</td>\n",
       "      <td>3</td>\n",
       "      <td>0.158261</td>\n",
       "      <td>0.111304</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>0.045217</td>\n",
       "      <td>0.012174</td>\n",
       "      <td>0.048696</td>\n",
       "      <td>0.006957</td>\n",
       "      <td>0.024348</td>\n",
       "      <td>0.114783</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.012174</td>\n",
       "      <td>0.026087</td>\n",
       "      <td>0.017391</td>\n",
       "      <td>0.024348</td>\n",
       "      <td>0.151304</td>\n",
       "      <td>0.050435</td>\n",
       "      <td>0.005217</td>\n",
       "      <td>0.128696</td>\n",
       "      <td>0.695652</td>\n",
       "      <td>0.464348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3517</th>\n",
       "      <td>1858-10-15</td>\n",
       "      <td>DSC</td>\n",
       "      <td>Daily Southern Cross</td>\n",
       "      <td>16</td>\n",
       "      <td>586.627451</td>\n",
       "      <td>39.0</td>\n",
       "      <td>672.0</td>\n",
       "      <td>633.0</td>\n",
       "      <td>29.450980</td>\n",
       "      <td>579.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Correspondence.</td>\n",
       "      <td>To the Members of the Southern Division. Gentl...</td>\n",
       "      <td>LetterToEditor</td>\n",
       "      <td>18</td>\n",
       "      <td>To the Members of the Southern Division Gentle...</td>\n",
       "      <td>448</td>\n",
       "      <td>573</td>\n",
       "      <td>22</td>\n",
       "      <td>350</td>\n",
       "      <td>191</td>\n",
       "      <td>4.149554</td>\n",
       "      <td>2306</td>\n",
       "      <td>39</td>\n",
       "      <td>52</td>\n",
       "      <td>97</td>\n",
       "      <td>26</td>\n",
       "      <td>10</td>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>61</td>\n",
       "      <td>24</td>\n",
       "      <td>10</td>\n",
       "      <td>23</td>\n",
       "      <td>15</td>\n",
       "      <td>23</td>\n",
       "      <td>38</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0.087054</td>\n",
       "      <td>0.116071</td>\n",
       "      <td>0.216518</td>\n",
       "      <td>0.058036</td>\n",
       "      <td>0.022321</td>\n",
       "      <td>0.104911</td>\n",
       "      <td>0.002232</td>\n",
       "      <td>0.053571</td>\n",
       "      <td>0.136161</td>\n",
       "      <td>0.053571</td>\n",
       "      <td>0.022321</td>\n",
       "      <td>0.051339</td>\n",
       "      <td>0.033482</td>\n",
       "      <td>0.051339</td>\n",
       "      <td>0.084821</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.013393</td>\n",
       "      <td>0.049107</td>\n",
       "      <td>0.781250</td>\n",
       "      <td>0.426339</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3518 rows × 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           date newspaper_id             newspaper  article_id  \\\n",
       "0    1878-10-26        KUMAT          Kumara Times           1   \n",
       "1    1878-10-26        KUMAT          Kumara Times           3   \n",
       "2    1878-10-26        KUMAT          Kumara Times           4   \n",
       "3    1878-10-26        KUMAT          Kumara Times           5   \n",
       "4    1878-10-26        KUMAT          Kumara Times           6   \n",
       "...         ...          ...                   ...         ...   \n",
       "3513 1858-10-15          DSC  Daily Southern Cross           8   \n",
       "3514 1858-10-15          DSC  Daily Southern Cross           9   \n",
       "3515 1858-10-15          DSC  Daily Southern Cross          10   \n",
       "3516 1858-10-15          DSC  Daily Southern Cross          12   \n",
       "3517 1858-10-15          DSC  Daily Southern Cross          16   \n",
       "\n",
       "      avg_line_width  min_line_width  max_line_width  line_width_range  \\\n",
       "0         452.272727           282.0           512.0             230.0   \n",
       "1         429.500000            95.0           515.0             420.0   \n",
       "2         469.765854            64.0           589.0             525.0   \n",
       "3         480.044444           109.0           599.0             490.0   \n",
       "4         461.769231            51.0           531.0             480.0   \n",
       "...              ...             ...             ...               ...   \n",
       "3513      587.000000           508.0           666.0             158.0   \n",
       "3514      536.000000           404.0           668.0             264.0   \n",
       "3515      656.030075            18.0           704.0             686.0   \n",
       "3516      612.764706            96.0           704.0             608.0   \n",
       "3517      586.627451            39.0           672.0             633.0   \n",
       "\n",
       "      avg_line_offset  max_line_offset  min_line_offset  \\\n",
       "0           33.090909            174.0              0.0   \n",
       "1           22.000000            104.0              0.0   \n",
       "2           22.575610            378.0              0.0   \n",
       "3           18.183333            244.0              0.0   \n",
       "4           22.745562            363.0              0.0   \n",
       "...               ...              ...              ...   \n",
       "3513        12.000000             24.0              0.0   \n",
       "3514        12.500000             25.0              0.0   \n",
       "3515        35.338346            368.0              0.0   \n",
       "3516        21.176471             67.0              0.0   \n",
       "3517        29.450980            579.0              0.0   \n",
       "\n",
       "                                           title  \\\n",
       "0                                    MAILS CLOSE   \n",
       "1                              LATEST TELEGRAMS.   \n",
       "2                              GENERAL ASSEMBLY.   \n",
       "3     ADDITIONAL NEWS BY THE SAN FRANCISCO MAIL.   \n",
       "4                              GENERAL ASSEMBLY.   \n",
       "...                                          ...   \n",
       "3513                                       BIRTH   \n",
       "3514                                       DIED.   \n",
       "3515                                THE COUNCIL.   \n",
       "3516                      PRESBYTERY OF AUCKLAND   \n",
       "3517                             Correspondence.   \n",
       "\n",
       "                                                   text           genre  \\\n",
       "0     For the United Kingdom, Continent of Europe, a...          Notice   \n",
       "1     : [PRESS AGENCY.] i : ;; BtUPP, October 25. / ...    FamilyNotice   \n",
       "2     Continued from 4th page. [press agency.] .;-•'...          Report   \n",
       "3     ■ f > • ; •* .■<■ j.f> '■' EUROPEAN ;/ ■ LoNi>...            News   \n",
       "4     'pPKSSS AGENCY.] LEGISLATIVE COUNCIL. Wellingt...          Report   \n",
       "...                                                 ...             ...   \n",
       "3513  At Wesley College. Auckland, on the 12th Octob...    FamilyNotice   \n",
       "3514  On Monday, the 11th instant, Mary, second daug...    FamilyNotice   \n",
       "3515  We have always maintained, that of the two bra...         Opinion   \n",
       "3516  , The principal meeting for the year of this C...          Report   \n",
       "3517  To the Members of the Southern Division. Gentl...  LetterToEditor   \n",
       "\n",
       "      sentence_count                                         clean_text  \\\n",
       "0                  3  For the United Kingdom Continent of Europe and...   \n",
       "1                  5  PRESS AGENCY i BtUPP October 25 Arrived Stella...   \n",
       "2                 32  Continued from 4th page press agency - J LEGIS...   \n",
       "3                 42  f j f EUROPEAN LoNi oii Septemb r 30i v - In C...   \n",
       "4                 39  pPKSSS AGENCY LEGISLATIVE COUNCIL Wellington O...   \n",
       "...              ...                                                ...   \n",
       "3513               3  At Wesley College Auckland on the 12th October...   \n",
       "3514               1  On Monday the 11th instant Mary second daughte...   \n",
       "3515              36  We have always maintained that of the two bran...   \n",
       "3516              22  The principal meeting for the year of this Cou...   \n",
       "3517              18  To the Members of the Southern Division Gentle...   \n",
       "\n",
       "      word_count  syll_count  polysyll_count  monosyll_count  stopwords_count  \\\n",
       "0             57          87               7              35               15   \n",
       "1             46          72               5              30               10   \n",
       "2           1131        1628             104             794              337   \n",
       "3           1079        1576             103             744              370   \n",
       "4           1043        1518             115             711              373   \n",
       "...          ...         ...             ...             ...              ...   \n",
       "3513          18          24               1              13                6   \n",
       "3514          14          18               0              10                2   \n",
       "3515        1088        1580             103             750              536   \n",
       "3516         575         845              74             400              267   \n",
       "3517         448         573              22             350              191   \n",
       "\n",
       "      avg_word_length  char_count  propn_count  verb_count  noun_count  \\\n",
       "0            4.620690         325           25           4           7   \n",
       "1            4.098039         259           16           3          12   \n",
       "2            4.469317        6327          298         123         235   \n",
       "3            4.538462        6119          255         111         219   \n",
       "4            4.614573        5855          254         121         175   \n",
       "...               ...         ...          ...         ...         ...   \n",
       "3513         3.944444          88            7           0           3   \n",
       "3514         4.571429          77            4           1           3   \n",
       "3515         4.573921        6069           69         123         203   \n",
       "3516         4.680556        3271           91          64          92   \n",
       "3517         4.149554        2306           39          52          97   \n",
       "\n",
       "      adj_count  nums_count  pron_count  nnps_count  vb_count  nn_count  \\\n",
       "0             1           3           0           2         2         5   \n",
       "1             1           2           1           0         0        12   \n",
       "2            40          36          27           7        44       185   \n",
       "3            56          29          40           8        12       174   \n",
       "4            37          25          35           9        39       132   \n",
       "...         ...         ...         ...         ...       ...       ...   \n",
       "3513          1           0           0           0         0         3   \n",
       "3514          2           1           0           0         0         2   \n",
       "3515         77           6          97           4        54       163   \n",
       "3516         26           7          28           4        14        66   \n",
       "3517         26          10          47           1        24        61   \n",
       "\n",
       "      jj_count  cd_count  prp_count  rb_count  cc_count  nnp_count  vbd_count  \\\n",
       "0            1         3          0         1         3         23          1   \n",
       "1            1         2          1         1         0         16          2   \n",
       "2           35        36         22        16        22        292         51   \n",
       "3           55        29         21        14        37        247         46   \n",
       "4           37        25         26        17        23        245         55   \n",
       "...        ...       ...        ...       ...       ...        ...        ...   \n",
       "3513         1         0          0         0         0          7          0   \n",
       "3514         2         1          0         0         0          4          0   \n",
       "3515        70         6         53        55        38         65         27   \n",
       "3516        25         7         15        10        14         87         29   \n",
       "3517        24        10         23        15        23         38          7   \n",
       "\n",
       "      vbz_count  propn_freq  verb_freq  noun_freq  adj_freq  nums_freq  \\\n",
       "0             0    0.438596   0.070175   0.122807  0.017544   0.052632   \n",
       "1             1    0.347826   0.065217   0.260870  0.021739   0.043478   \n",
       "2             5    0.263484   0.108753   0.207781  0.035367   0.031830   \n",
       "3            27    0.236330   0.102873   0.202966  0.051900   0.026877   \n",
       "4             2    0.243528   0.116012   0.167785  0.035475   0.023969   \n",
       "...         ...         ...        ...        ...       ...        ...   \n",
       "3513          0    0.388889   0.000000   0.166667  0.055556   0.000000   \n",
       "3514          0    0.285714   0.071429   0.214286  0.142857   0.071429   \n",
       "3515         33    0.063419   0.113051   0.186581  0.070772   0.005515   \n",
       "3516          3    0.158261   0.111304   0.160000  0.045217   0.012174   \n",
       "3517          6    0.087054   0.116071   0.216518  0.058036   0.022321   \n",
       "\n",
       "      pron_freq  nnps_freq   vb_freq   nn_freq   jj_freq   cd_freq  prp_freq  \\\n",
       "0      0.000000   0.035088  0.035088  0.087719  0.017544  0.052632  0.000000   \n",
       "1      0.021739   0.000000  0.000000  0.260870  0.021739  0.043478  0.021739   \n",
       "2      0.023873   0.006189  0.038904  0.163572  0.030946  0.031830  0.019452   \n",
       "3      0.037071   0.007414  0.011121  0.161260  0.050973  0.026877  0.019462   \n",
       "4      0.033557   0.008629  0.037392  0.126558  0.035475  0.023969  0.024928   \n",
       "...         ...        ...       ...       ...       ...       ...       ...   \n",
       "3513   0.000000   0.000000  0.000000  0.166667  0.055556  0.000000  0.000000   \n",
       "3514   0.000000   0.000000  0.000000  0.142857  0.142857  0.071429  0.000000   \n",
       "3515   0.089154   0.003676  0.049632  0.149816  0.064338  0.005515  0.048713   \n",
       "3516   0.048696   0.006957  0.024348  0.114783  0.043478  0.012174  0.026087   \n",
       "3517   0.104911   0.002232  0.053571  0.136161  0.053571  0.022321  0.051339   \n",
       "\n",
       "       rb_freq   cc_freq  nnp_freq  vbd_freq  vbz_freq  polysyll_freq  \\\n",
       "0     0.017544  0.052632  0.403509  0.017544  0.000000       0.122807   \n",
       "1     0.021739  0.000000  0.347826  0.043478  0.021739       0.108696   \n",
       "2     0.014147  0.019452  0.258179  0.045093  0.004421       0.091954   \n",
       "3     0.012975  0.034291  0.228916  0.042632  0.025023       0.095459   \n",
       "4     0.016299  0.022052  0.234899  0.052733  0.001918       0.110259   \n",
       "...        ...       ...       ...       ...       ...            ...   \n",
       "3513  0.000000  0.000000  0.388889  0.000000  0.000000       0.055556   \n",
       "3514  0.000000  0.000000  0.285714  0.000000  0.000000       0.000000   \n",
       "3515  0.050551  0.034926  0.059743  0.024816  0.030331       0.094669   \n",
       "3516  0.017391  0.024348  0.151304  0.050435  0.005217       0.128696   \n",
       "3517  0.033482  0.051339  0.084821  0.015625  0.013393       0.049107   \n",
       "\n",
       "      monosyll_freq  stopword_freq  \n",
       "0          0.614035       0.263158  \n",
       "1          0.652174       0.217391  \n",
       "2          0.702034       0.297966  \n",
       "3          0.689527       0.342910  \n",
       "4          0.681687       0.357622  \n",
       "...             ...            ...  \n",
       "3513       0.722222       0.333333  \n",
       "3514       0.714286       0.142857  \n",
       "3515       0.689338       0.492647  \n",
       "3516       0.695652       0.464348  \n",
       "3517       0.781250       0.426339  \n",
       "\n",
       "[3518 rows x 60 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "display(features_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save dataframe for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataframe for later use \n",
    "# https://stackoverflow.com/questions/17098654/how-to-reversibly-store-and-load-a-pandas-dataframe-to-from-disk\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# Uncomment below to use date and time for filename\n",
    "\n",
    "# time_now = datetime.now()\n",
    "# file_date = time_now.strftime(\"%Y%m%d_%H%M%S\")\n",
    "# clean_df.to_pickle(f\"{file_date}_PP_df.pkl\")\n",
    "\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# Uncomment below to use custom filename\n",
    "\n",
    "pkl_filename = '20220219_PP_3518articles_features_exclTFIDF' # Change filename here\n",
    "features_df.to_pickle(f\"{pkl_filename}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
