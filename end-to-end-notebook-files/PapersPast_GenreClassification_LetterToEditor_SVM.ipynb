{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Papers Past Genre Classification\n",
    "# End-to-End Notebook: Letter to Editor\n",
    "---\n",
    "\n",
    "## Support Vector Machine\n",
    "\n",
    "* This notebook processes the raw Papers Past METS/ALTO XML files (saved in tar.gz format by newspaper and year) to return a Pandas dataframe of articles sorted by the confidence that they are letters to the editor (distance from the decision boundary). \n",
    "* The final dataframe is exported as a csv file with links to the online Papers Past newspaper issue for each article. This allows you to view the scanned image of the original article.   \n",
    "* The given number of newspaper issues are randomly selected, with the option to set a seed for reproducibility. \n",
    "* This version of the notebook includes TF-IDF feature extraction, which is computationally expensive and may result in out of memory errors for larger sample sizes. The alternative notebook, using a model that excludes the TF-IDF feature, performs slightly better for the letter to the editor genre and is recommended. This version is included for completeness and consistency with the other genres.\n",
    "* Note that the sampling and feature extraction processes can take quite a while to run. For example, a sample of 300 newspaper issues could take up to an hour on an average laptop and 6,000 issues (about 2% of the entire dataset) could take 20+ hours. You should change your computer's settings to prevent it going to sleep or powering off while the notebook is running.  \n",
    "* Dataframes are saved in pickle format following sampling and feature extraction so that if an error occurs or the notebook doesn't run all the way through for some reason, the saved dataframe can be loaded and you can restart the process from that point. These points in the notebook are highlighted in orange. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Owner\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "import re\n",
    "import os\n",
    "import statistics\n",
    "import tarfile\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "import pickle\n",
    "import time\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "\n",
    "import spacy\n",
    "import math\n",
    "import textstat\n",
    "import textfeatures as tf\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# Features\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Classifiers\n",
    "from sklearn.svm import SVC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load mets namespace\n",
    "# http://www.loc.gov/standards/mets/namespace.html\n",
    "\n",
    "NS = {'mets':'http://www.loc.gov/METS/'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <h3>Set the variables in the cell below before running all cells</h3>\n",
    "    <p>\n",
    "        <li>Select a random seed for reproducibility.\n",
    "        <li>Provide the filepath for the newspaper-year tar.gz files.\n",
    "        <li>Select the number of issues to sample.\n",
    "        <li>Provide a filename for saving dataframes and exporting the final csv file\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter a random seed for reproducibility of results\n",
    "random.seed(a=10)\n",
    "\n",
    "# Set the top level directory to sample from\n",
    "dir_name = 'E:/PapersPast_OpenData'\n",
    "\n",
    "# Set the number of issues to be sampled\n",
    "num_issues = 10\n",
    "\n",
    "# Filename for saved files \n",
    "# Do not include filetype suffix such as .csv - this is added as part of the code\n",
    "export_filename = \"20220304_NewLetter2Ed_10issues_seed10_df\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the saved model and feature set\n",
    "\n",
    "* The following cells load the saved model and the relevant feature set. \n",
    "* Make sure that the model file is saved in the same location as this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filename of the saved scikit-learn model (should not need to be changed)\n",
    "filename = 'pp_svm_letter2ed.sav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature set (should not need to be changed)\n",
    "features = [\"propn_freq\", \n",
    "            \"verb_freq\", \n",
    "            \"noun_freq\", \n",
    "            \"adj_freq\", \n",
    "            \"nums_freq\", \n",
    "            \"pron_freq\", \n",
    "            \"nnps_freq\", \n",
    "            \"vb_freq\", \n",
    "            \"nn_freq\", \n",
    "            \"jj_freq\", \n",
    "            \"cd_freq\", \n",
    "            \"prp_freq\", \n",
    "            \"rb_freq\", \n",
    "            \"cc_freq\", \n",
    "            \"nnp_freq\", \n",
    "            \"vbd_freq\", \n",
    "            \"vbz_freq\", \n",
    "            \"stopword_freq\", \n",
    "            \"avg_line_offset\", \n",
    "            \"max_line_offset\", \n",
    "            \"avg_line_width\", \n",
    "            \"min_line_width\", \n",
    "            \"max_line_width\", \n",
    "            \"line_width_range\", \n",
    "            \"polysyll_freq\", \n",
    "            \"monosyll_freq\", \n",
    "            \"sentence_count\", \n",
    "            \"word_count\", \n",
    "            \"avg_word_length\", \n",
    "            \"char_count\", \n",
    "            \"tf_idf_sum\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data extraction and page layout features\n",
    "\n",
    "The following cells extract the article details and page layout features from the tar.gz files and return a Pandas dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tarball(filepath):\n",
    "    \"\"\"\n",
    "    Given path to tarball, open and return dictionary containing \n",
    "    article codes as keys and texts (as list of strings for each block) as values.\n",
    "    \"\"\"\n",
    "    newspaper_year = tarfile.open(filepath, mode='r')\n",
    "\n",
    "    # Next, return the members of the archive as a list of TarInfo objects. \n",
    "    # The list has the same order as the members in the archive.\n",
    "    # https://docs.python.org/3/library/tarfile.html\n",
    "    files = newspaper_year.getmembers() \n",
    "\n",
    "    issues = collect_issues(files)\n",
    "    selected_issue = select_random_issue(issues)\n",
    "    mets_tarinfo = issues[selected_issue][-1]\n",
    "    pages_tarinfo = issues[selected_issue][0:-1]\n",
    "    article_codes = mets2codes(mets_tarinfo, newspaper_year)\n",
    "    articles = codes2texts(article_codes, pages_tarinfo, newspaper_year, selected_issue)\n",
    "\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_issues(files):\n",
    "    \"\"\"\n",
    "    Given list of files in tarball, return a dictionary keyed\n",
    "    by the issue code with list of xml files of form [0001.xml, ..., mets.xml]\n",
    "    as values.\n",
    "    \"\"\"\n",
    "    issues = {}\n",
    "    issue_code = ''\n",
    "\n",
    "    for file in files:\n",
    "        match = re.search(\"[A-Z]*_\\d{8}$\", file.name)\n",
    "        if match:\n",
    "            issue_code = match.group(0)\n",
    "        if file.name.endswith('.xml'):\n",
    "            xml_list = issues.get(issue_code, [])\n",
    "            xml_list.append(file)\n",
    "            issues[issue_code] = xml_list\n",
    "\n",
    "    return issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_random_issue(issues):\n",
    "    \"\"\"\n",
    "    Select a random issue from a given dictionary of the all of a newspaper's issues for one year \n",
    "    (with the issue code as the key and the list of XML files as the elements).\n",
    "    \"\"\"\n",
    "    selected_issue = random.choice(list(issues))\n",
    "\n",
    "    return selected_issue "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mets2codes(mets_tarinfo, newspaper_year):\n",
    "    \"\"\"\n",
    "    Given mets as tarinfo, return text block codes for articles\n",
    "    contained in mets file. Edited for processing with tarfile\n",
    "    object newspaper_year.\n",
    "\n",
    "    Returns dictionary of article codes as keys,\n",
    "    with a 2-tuple containing the article title\n",
    "    and a list of corresponding text block codes as values.\n",
    "    \"\"\"\n",
    "    with newspaper_year.extractfile(mets_tarinfo) as file:\n",
    "        text = file.read()\n",
    "\n",
    "    art_dict = mets2codes_inner(text, newspaper_year)\n",
    "\n",
    "    return art_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mets2codes_inner(text, newspaper_year):\n",
    "    \"\"\"\n",
    "    Given METS file as text string, return a dictionary of\n",
    "    articles, with article codes as keys and, as values, tuples containing\n",
    "    the corresponding article title and a list of text blocks from the\n",
    "    corresponding ALTO files as values.\n",
    "    \"\"\"\n",
    "    mets_root = ET.fromstring(text) # Loads the mets xml file (which comes into the function as a string)\n",
    "    logical_structure = mets_root.find(\"./mets:structMap[@LABEL='Logical Structure']\", NS) # Finds the \"logical structure\" part of the file, which lists all the articles and the blocks they contain.\n",
    "    articles_div = logical_structure.findall(\".//mets:div[@TYPE='ARTICLE']\", NS) # This returns all of the \"div\" elements in the logical structure part of the xml that have the attribute \"TYPE='ARTICLE'\". This is where we lose the advertisements.\n",
    "\n",
    "    art_dict = {} # This is an empty dictionary which will collect what we need from the mets file. It will have articles ids as keys and have the ids of the text blocks which are part of the article as values.\n",
    "    for article in articles_div:\n",
    "        \n",
    "        attributes = article.attrib\n",
    "        article_id = attributes['DMDID']\n",
    "        article_title = attributes.get('LABEL', 'UNTITLED')\n",
    "\n",
    "        text_blocks = article.findall(\".//mets:div[@TYPE='TEXT']\", NS)\n",
    "        block_ids = []\n",
    "        for block in text_blocks:\n",
    "            try:\n",
    "                areas = block.findall(\".//mets:area\", NS)\n",
    "                for area in areas:\n",
    "                    block_id = area.attrib['BEGIN']\n",
    "                    block_ids.append(block_id)\n",
    "            except AttributeError:\n",
    "                print(f'Error in {newspaper_year}')\n",
    "        \n",
    "        art_dict[article_id] = (article_title, block_ids)\n",
    "\n",
    "    mets_root.clear() # When processing lots of these, we want to free up memory.\n",
    "\n",
    "    # print(art_dict)\n",
    "    return art_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def codes2texts(article_codes, pages_tarinfo, newspaper_year, selected_issue):\n",
    "    \"\"\"\n",
    "    Given article codes, the issue pages as tar info objects, \n",
    "    the newspaper year and the issue code, return a dictionary\n",
    "    with article codes as keys and a list of text blocks as\n",
    "    strings as values.\n",
    "    \"\"\"\n",
    "    page_roots = parse_pages_tar(pages_tarinfo, newspaper_year)\n",
    "    # page_roots returns a dictionary with pages numbers (of form 'P1'\n",
    "    # etc...) as keys and the XML roots of the pages as values.\n",
    "\n",
    "    texts_dict = codes2texts_inner(article_codes, page_roots, selected_issue)\n",
    "\n",
    "    # Clear roots.\n",
    "    for i in range(len(page_roots)):\n",
    "        k, v = page_roots.popitem()\n",
    "        v.clear()\n",
    "\n",
    "    return texts_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_pages_tar(pages, newspaper_year):\n",
    "    \"\"\"\n",
    "    Given iterable of paths to page files, return\n",
    "    dictionary with 'P1', 'P2', etc as keys, and the\n",
    "    root element of each page as values.\n",
    "    \"\"\"\n",
    "    page_roots = {}\n",
    "    for i, page in enumerate(pages):\n",
    "        with newspaper_year.extractfile(page) as f:\n",
    "            text = f.read()\n",
    "        root = ET.fromstring(text)\n",
    "        page_roots[f'P{i+1}'] = root\n",
    "\n",
    "    return page_roots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def codes2texts_inner(article_codes, page_roots, selected_issue):\n",
    "    \"\"\"\n",
    "    Given XML roots of ALTO pages and collection of article codes\n",
    "    and corresponding blocks, return a dictionary with article codes\n",
    "    as keys and a list of text blocks as strings as values.\n",
    "    \"\"\"\n",
    "    texts_dict = {}  \n",
    "    \n",
    "    for article_id in article_codes.keys():\n",
    "        title, blocks = article_codes[article_id]\n",
    "        text = []\n",
    "        line_widths = []\n",
    "        line_hpos = []\n",
    "        line_offsets = []\n",
    "        \n",
    "        for block in blocks:\n",
    "\n",
    "            # The block ids have page numbers as part. We collect the page number.\n",
    "            end_loc = block.find('_')\n",
    "            page_no = block[0:end_loc]\n",
    "\n",
    "            # Collect the relevant page (the alto file) for the block.\n",
    "            page_root = page_roots[page_no]\n",
    "\n",
    "            # Collect the specific block from the page and identify the desired elements \n",
    "            # (strings, lines, horizontal position etc.)\n",
    "            xml_block = page_root.find(f\".//TextBlock[@ID='{block}']\")\n",
    "\n",
    "            block_strings = xml_block.findall('.//String')\n",
    "            block_lines = xml_block.findall('.//TextLine')\n",
    "            block_hpos = int(xml_block.get(\"HPOS\"))\n",
    "\n",
    "            # Collect the information we want from the blocks.\n",
    "            block_as_string = process_block(block_strings)\n",
    "            block_line_widths = process_block_lines(block_lines)\n",
    "            block_line_hpos = process_lines_hpos(block_lines)\n",
    "        \n",
    "            block_line_offsets = [hpos - block_hpos for hpos in block_line_hpos]\n",
    "            \n",
    "            text.append(block_as_string)\n",
    "            line_widths.extend(block_line_widths)\n",
    "            line_hpos.extend(block_line_hpos)\n",
    "            line_offsets.extend(block_line_offsets)\n",
    "\n",
    "        text = ' '.join(text)\n",
    "        issue_article_id = selected_issue + '_' + article_id[7:]\n",
    "        # texts_dict[issue_article_id] = (title, text, line_widths, line_hpos)\n",
    "        texts_dict[issue_article_id] = (title, text, line_widths, line_hpos, line_offsets)\n",
    "\n",
    "    \n",
    "    return texts_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_block(block_strings):\n",
    "    \"\"\"\n",
    "    Given xml String elements from text block, return whole block\n",
    "    as single string.\n",
    "    \"\"\"\n",
    "    words = []\n",
    "    for s in block_strings:\n",
    "        words.append(s.attrib['CONTENT'])\n",
    "    total_string = ' '.join(words)\n",
    "\n",
    "    return total_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_block_lines(block_lines):\n",
    "    \"\"\"\n",
    "    Given xml TextLine elements from text block, return a list of the widths.\n",
    "    \"\"\"\n",
    "    line_w = []\n",
    "    \n",
    "    for line in block_lines:\n",
    "        line_w.append(int(line.attrib['WIDTH']))\n",
    "\n",
    "    return line_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_lines_hpos(block_lines):\n",
    "    \"\"\"\n",
    "    Given xml TextLine elements from text block, return a list of the\n",
    "    horizontal starting position of each line.\n",
    "    \"\"\"\n",
    "    line_hpos = []\n",
    "    \n",
    "    for line in block_lines:\n",
    "        line_hpos.append(int(line.attrib['HPOS']))\n",
    "\n",
    "    return line_hpos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_collect(filepath):\n",
    "    \"\"\"\n",
    "    Return dataframe for the selected newspaper/year.\n",
    "    \"\"\"\n",
    "    # print(f'Processing {path}')\n",
    "    try:\n",
    "        articles = process_tarball(filepath)\n",
    "        dataframe = pd.DataFrame.from_dict(\n",
    "            articles,\n",
    "            orient='index',\n",
    "            dtype = object,\n",
    "            # columns=['title', 'text', 'line_widths', 'line_hpos']\n",
    "            columns=['title', 'text', 'line_widths', 'line_hpos', 'line_offsets']\n",
    "            )\n",
    "    except:\n",
    "        print(f'Problem with {filepath}')\n",
    "        dataframe = None\n",
    "        \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code source: https://thispointer.com/python-how-to-get-list-of-files-in-directory-and-sub-directories/\n",
    "\n",
    "def get_files(dir_name):\n",
    "    \"\"\"\n",
    "    For the given path, get a list of all files in the directory tree\n",
    "    \"\"\"\n",
    "    # Create a list of files and sub directories \n",
    "    files_dir = os.listdir(dir_name)\n",
    "    file_list = list()\n",
    "    \n",
    "    # Iterate over all the entries\n",
    "    for item in files_dir:\n",
    "        \n",
    "        # Create full path\n",
    "        full_path = os.path.join(dir_name, item)\n",
    "        \n",
    "        # If entry is a directory then get the list of files in this directory \n",
    "        if os.path.isdir(full_path):\n",
    "            file_list = file_list + get_files(dir_name)\n",
    "        else:\n",
    "            file_list.append(full_path)\n",
    "                \n",
    "    return file_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_and_create(dir_name, num_issues):\n",
    "    \"\"\"\n",
    "    Randomly select a given number of issues\n",
    "    and return a dataframe.\n",
    "    \"\"\"\n",
    "\n",
    "    file_list = get_files(dir_name)\n",
    "    file_paths = []\n",
    "    single_dfs = [] # A list of the dataframes created for each newspaper issue\n",
    "\n",
    "    for random_selection in range(0, num_issues):\n",
    "        \n",
    "        selected_tar = random.choice(file_list)\n",
    "        file_paths.append(selected_tar)  \n",
    "    \n",
    "    for filepath in file_paths:\n",
    "        single_df = process_and_collect(filepath)\n",
    "        single_dfs.append(single_df)\n",
    "    \n",
    "    final_df = pd.concat(single_dfs, axis = 0)\n",
    "    final_df.reset_index(drop=False, inplace=True, col_level=0)\n",
    "        \n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_df(dir_name, num_issues):\n",
    "    \"\"\"\n",
    "    Given a directory for the Papers Past \n",
    "    open data (with newspaper-year files \n",
    "    in tar.gz format) and a number of issues \n",
    "    to randomly select, return a dataframe \n",
    "    of articles and features.\n",
    "    \"\"\"\n",
    "                \n",
    "    final_df = select_and_create(dir_name, num_issues)\n",
    "    \n",
    "    # Calculate features from the lists of line widths and positions\n",
    "    final_df['avg_line_width'] = pd.DataFrame(final_df['line_widths'].values.tolist()).mean(1)\n",
    "    final_df['max_line_width'] = pd.DataFrame(final_df['line_widths'].values.tolist()).max(1)\n",
    "    final_df['min_line_width'] = pd.DataFrame(final_df['line_widths'].values.tolist()).min(1)\n",
    "    final_df['line_width_range'] = final_df['max_line_width'] - final_df['min_line_width']\n",
    "    \n",
    "    # Line offsets relate to the difference between the starting horizontal position of each line compared to the block\n",
    "    final_df['avg_line_offset'] = pd.DataFrame(final_df['line_offsets'].values.tolist()).mean(1)\n",
    "    final_df['max_line_offset'] = pd.DataFrame(final_df['line_offsets'].values.tolist()).max(1)\n",
    "    final_df['min_line_offset'] = pd.DataFrame(final_df['line_offsets'].values.tolist()).min(1)\n",
    "       \n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem with E:/PapersPast_OpenData\\LT_1891.tar.gz\n",
      "Returned dataframe in 113.0783 seconds\n"
     ]
    }
   ],
   "source": [
    "# Create the final dataframe and measure time to load\n",
    "t1 = time.perf_counter()\n",
    "\n",
    "final_df = produce_df(dir_name, num_issues)\n",
    "\n",
    "t2 = time.perf_counter()\n",
    "\n",
    "print(f\"Returned dataframe in {t2 - t1:0.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "416"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for and remove any duplicate articles\n",
    "\n",
    "final_df = final_df.drop_duplicates(subset='index', keep=\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrangled_df(final_df):\n",
    "    \"\"\"\n",
    "    Given the combined final dataframe of Papers Past articles, \n",
    "    rename and reorder columns, and add the full newspaper name from\n",
    "    a given dictionary supplied as a csv file.\n",
    "    \"\"\"\n",
    "    # A dictionary of newspaper codes mapped to newspaper name and region is loaded\n",
    "    codes2newspaper = pd.read_csv('PP_Codes2Newspaper.csv', header=None, dtype={0: str}).set_index(0).squeeze().to_dict()\n",
    "    \n",
    "    # Separate features are extracted from the 'index' column\n",
    "    final_df['newspaper_id'] = final_df[\"index\"].str.extract(r\"([^_]*)\") # Extract the letters before the first underscore as Newspaper ID\n",
    "    final_df['date'] = final_df[\"index\"].str.extract(r\"(?<=\\_)(.*?)(?=\\_)\") # Extract the numbers between the underscores as date\n",
    "    final_df['article_id'] = final_df[\"index\"].str.extract(r\"(\\d+)(?!.*\\d)\") # Extract the numeric portion of the article ID\n",
    "    final_df.drop('index', inplace=True, axis=1) # Drop the index column\n",
    "    \n",
    "    # The Northern Advocate's code is NA so it comes through as nan. This is deleted and then replaced correctly in the dictionary\n",
    "    codes2newspaper = {key: value for key, value in codes2newspaper.items() if pd.notna(key)}\n",
    "    codes2newspaper['NA'] = 'Northern Advocate' \n",
    "    final_df['newspaper'] = final_df['newspaper_id'].map(codes2newspaper)\n",
    "    \n",
    "    # The data types of the columns are updated\n",
    "    final_df['date'] = pd.to_datetime(final_df['date'], format='%Y%m%d') \n",
    "    final_df['article_id'] = (final_df['article_id']).astype(int)\n",
    "    final_df['text'] = (final_df['text']).astype('string')\n",
    "    final_df['title'] = (final_df['title']).astype('string')\n",
    "    \n",
    "    final_df['newspaper_id'] = (final_df['newspaper_id']).astype('string')\n",
    "    final_df['newspaper'] = (final_df['newspaper']).astype('string')\n",
    "    \n",
    "    # Columns are reordered\n",
    "    new_order = [\"date\", \n",
    "                 \"newspaper_id\", \n",
    "                 \"newspaper\", \n",
    "                 \"article_id\", \n",
    "                 \"avg_line_width\", \n",
    "                 \"min_line_width\", \n",
    "                 \"max_line_width\", \n",
    "                 \"line_width_range\", \n",
    "                 \"avg_line_offset\", \n",
    "                 \"max_line_offset\", \n",
    "                 \"min_line_offset\", \n",
    "                 \"title\", \n",
    "                 \"text\"]\n",
    "    clean_df = final_df.reindex(columns = new_order)\n",
    "    \n",
    "    return clean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>newspaper_id</th>\n",
       "      <th>newspaper</th>\n",
       "      <th>article_id</th>\n",
       "      <th>avg_line_width</th>\n",
       "      <th>min_line_width</th>\n",
       "      <th>max_line_width</th>\n",
       "      <th>line_width_range</th>\n",
       "      <th>avg_line_offset</th>\n",
       "      <th>max_line_offset</th>\n",
       "      <th>min_line_offset</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1889-04-09</td>\n",
       "      <td>TEML</td>\n",
       "      <td>Temuka Leader</td>\n",
       "      <td>1</td>\n",
       "      <td>434.083871</td>\n",
       "      <td>74.0</td>\n",
       "      <td>517.0</td>\n",
       "      <td>443.0</td>\n",
       "      <td>35.425806</td>\n",
       "      <td>386.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>TELEGRAMS.</td>\n",
       "      <td>ENGLISH AND FOREIGN, London, April 4 Her Majes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1889-04-09</td>\n",
       "      <td>TEML</td>\n",
       "      <td>Temuka Leader</td>\n",
       "      <td>2</td>\n",
       "      <td>442.789474</td>\n",
       "      <td>97.0</td>\n",
       "      <td>514.0</td>\n",
       "      <td>417.0</td>\n",
       "      <td>26.894737</td>\n",
       "      <td>385.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ENGLISH POLITICS.</td>\n",
       "      <td>London, April 4, The Unionists have adopted Mr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1889-04-09</td>\n",
       "      <td>TEML</td>\n",
       "      <td>Temuka Leader</td>\n",
       "      <td>3</td>\n",
       "      <td>415.200000</td>\n",
       "      <td>354.0</td>\n",
       "      <td>480.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>38.200000</td>\n",
       "      <td>123.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>YELLOW FEVER AT RIO.</td>\n",
       "      <td>Rio de Janeiro, April 3. Deaths from yellow fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1889-04-09</td>\n",
       "      <td>TEML</td>\n",
       "      <td>Temuka Leader</td>\n",
       "      <td>4</td>\n",
       "      <td>446.551724</td>\n",
       "      <td>93.0</td>\n",
       "      <td>517.0</td>\n",
       "      <td>424.0</td>\n",
       "      <td>42.017241</td>\n",
       "      <td>391.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>THE SAMOAN DISASTER.</td>\n",
       "      <td>Sydney, April 5. The National Shipwreck Belief...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1889-04-09</td>\n",
       "      <td>TEML</td>\n",
       "      <td>Temuka Leader</td>\n",
       "      <td>5</td>\n",
       "      <td>475.102564</td>\n",
       "      <td>82.0</td>\n",
       "      <td>515.0</td>\n",
       "      <td>433.0</td>\n",
       "      <td>11.910256</td>\n",
       "      <td>249.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>PARNELL V. THE TIMES.</td>\n",
       "      <td>London, April 5. In h'3 address before the Tim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>1888-01-19</td>\n",
       "      <td>PATM</td>\n",
       "      <td>Patea Mail</td>\n",
       "      <td>18</td>\n",
       "      <td>573.275862</td>\n",
       "      <td>211.0</td>\n",
       "      <td>631.0</td>\n",
       "      <td>420.0</td>\n",
       "      <td>50.482759</td>\n",
       "      <td>90.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>“HAD”</td>\n",
       "      <td>An amusing incident, which oc curred not a hun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>1888-01-19</td>\n",
       "      <td>PATM</td>\n",
       "      <td>Patea Mail</td>\n",
       "      <td>19</td>\n",
       "      <td>563.437500</td>\n",
       "      <td>21.0</td>\n",
       "      <td>631.0</td>\n",
       "      <td>610.0</td>\n",
       "      <td>41.776786</td>\n",
       "      <td>110.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>A REMARKABLE CASE.</td>\n",
       "      <td>Under the above heading the Doncaster Dejgorfe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>1888-01-19</td>\n",
       "      <td>PATM</td>\n",
       "      <td>Patea Mail</td>\n",
       "      <td>20</td>\n",
       "      <td>500.547486</td>\n",
       "      <td>62.0</td>\n",
       "      <td>583.0</td>\n",
       "      <td>521.0</td>\n",
       "      <td>19.122905</td>\n",
       "      <td>165.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>LITERATURE.</td>\n",
       "      <td>UNDER THE SHADOW. CHAPTER Xl. Continued. Pale ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>1888-01-19</td>\n",
       "      <td>PATM</td>\n",
       "      <td>Patea Mail</td>\n",
       "      <td>21</td>\n",
       "      <td>480.777778</td>\n",
       "      <td>52.0</td>\n",
       "      <td>577.0</td>\n",
       "      <td>525.0</td>\n",
       "      <td>30.811966</td>\n",
       "      <td>376.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>CABLE NEWS.</td>\n",
       "      <td>London*, January 14. Consols, 1024. Chilian co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>1888-01-19</td>\n",
       "      <td>PATM</td>\n",
       "      <td>Patea Mail</td>\n",
       "      <td>22</td>\n",
       "      <td>520.266667</td>\n",
       "      <td>104.0</td>\n",
       "      <td>582.0</td>\n",
       "      <td>478.0</td>\n",
       "      <td>35.777778</td>\n",
       "      <td>274.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>AUSTRALIAN.</td>\n",
       "      <td>S ith: January 17. The; cricke' match b-tween ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>416 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          date newspaper_id      newspaper  article_id  avg_line_width  \\\n",
       "0   1889-04-09         TEML  Temuka Leader           1      434.083871   \n",
       "1   1889-04-09         TEML  Temuka Leader           2      442.789474   \n",
       "2   1889-04-09         TEML  Temuka Leader           3      415.200000   \n",
       "3   1889-04-09         TEML  Temuka Leader           4      446.551724   \n",
       "4   1889-04-09         TEML  Temuka Leader           5      475.102564   \n",
       "..         ...          ...            ...         ...             ...   \n",
       "411 1888-01-19         PATM     Patea Mail          18      573.275862   \n",
       "412 1888-01-19         PATM     Patea Mail          19      563.437500   \n",
       "413 1888-01-19         PATM     Patea Mail          20      500.547486   \n",
       "414 1888-01-19         PATM     Patea Mail          21      480.777778   \n",
       "415 1888-01-19         PATM     Patea Mail          22      520.266667   \n",
       "\n",
       "     min_line_width  max_line_width  line_width_range  avg_line_offset  \\\n",
       "0              74.0           517.0             443.0        35.425806   \n",
       "1              97.0           514.0             417.0        26.894737   \n",
       "2             354.0           480.0             126.0        38.200000   \n",
       "3              93.0           517.0             424.0        42.017241   \n",
       "4              82.0           515.0             433.0        11.910256   \n",
       "..              ...             ...               ...              ...   \n",
       "411           211.0           631.0             420.0        50.482759   \n",
       "412            21.0           631.0             610.0        41.776786   \n",
       "413            62.0           583.0             521.0        19.122905   \n",
       "414            52.0           577.0             525.0        30.811966   \n",
       "415           104.0           582.0             478.0        35.777778   \n",
       "\n",
       "     max_line_offset  min_line_offset                  title  \\\n",
       "0              386.0              0.0             TELEGRAMS.   \n",
       "1              385.0              0.0      ENGLISH POLITICS.   \n",
       "2              123.0              0.0   YELLOW FEVER AT RIO.   \n",
       "3              391.0              0.0   THE SAMOAN DISASTER.   \n",
       "4              249.0              0.0  PARNELL V. THE TIMES.   \n",
       "..               ...              ...                    ...   \n",
       "411             90.0              0.0                  “HAD”   \n",
       "412            110.0              0.0     A REMARKABLE CASE.   \n",
       "413            165.0              0.0            LITERATURE.   \n",
       "414            376.0              0.0            CABLE NEWS.   \n",
       "415            274.0              7.0            AUSTRALIAN.   \n",
       "\n",
       "                                                  text  \n",
       "0    ENGLISH AND FOREIGN, London, April 4 Her Majes...  \n",
       "1    London, April 4, The Unionists have adopted Mr...  \n",
       "2    Rio de Janeiro, April 3. Deaths from yellow fe...  \n",
       "3    Sydney, April 5. The National Shipwreck Belief...  \n",
       "4    London, April 5. In h'3 address before the Tim...  \n",
       "..                                                 ...  \n",
       "411  An amusing incident, which oc curred not a hun...  \n",
       "412  Under the above heading the Doncaster Dejgorfe...  \n",
       "413  UNDER THE SHADOW. CHAPTER Xl. Continued. Pale ...  \n",
       "414  London*, January 14. Consols, 1024. Chilian co...  \n",
       "415  S ith: January 17. The; cricke' match b-tween ...  \n",
       "\n",
       "[416 rows x 13 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clean_df = wrangled_df(final_df)\n",
    "display(clean_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df.to_pickle(f\"{export_filename}_SAMPLE.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <h3>Reload saved dataframe (if required)</h3>\n",
    "    <p>The previously saved dataframe of sample results and article layout features can be reloaded by uncommenting and running the following cell.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_df = pd.read_pickle(f\"{export_filename}_SAMPLE.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction: linguistic features and text statistics\n",
    "\n",
    "The following cells extract parts-of-speech and text statistic features and add them to the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaner(df, column_name):\n",
    "    \"\"\"\n",
    "    Remove unnecessary symbols to create a clean text column from the original dataframe column using a regex.\n",
    "    \"\"\"\n",
    "    # A column of sentence count is added to the dataframe before punctuation is removed.\n",
    "    df['sentence_count'] = df[column_name].apply(lambda x: textstat.sentence_count(x))\n",
    "\n",
    "    # Regex pattern for only alphanumeric, hyphenated text\n",
    "    pattern = re.compile(r\"[A-Za-z0-9\\-]{1,50}\")\n",
    "    df['clean_text'] = df[column_name].str.findall(pattern).str.join(' ')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = cleaner(clean_df, 'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_propn_spacy(doc):\n",
    "    \"\"\"\n",
    "    Given a Spacy doc object return counts of the \n",
    "    following parts-of-speech: proper nouns.\n",
    "    \n",
    "    Optional: uncomment the code lines to collect and \n",
    "    print a list of the tagged words.\n",
    "    \"\"\"\n",
    "    count_propn = 0\n",
    "    # propn_list = []\n",
    "\n",
    "    for token in doc:\n",
    "        if token.pos_ == 'PROPN':\n",
    "            count_propn += 1\n",
    "        \n",
    "    return count_propn \n",
    "\n",
    "\n",
    "def count_verb_spacy(doc):\n",
    "    \"\"\"\n",
    "    Given a Spacy doc object return counts of the \n",
    "    following parts-of-speech: verbs.\n",
    "    \n",
    "    Optional: uncomment the code lines to collect and \n",
    "    print a list of the tagged words.\n",
    "    \"\"\"\n",
    "    count_verb = 0\n",
    "    # verb_list = []\n",
    "\n",
    "    for token in doc:\n",
    "        if token.pos_ == 'VERB':\n",
    "            count_verb += 1\n",
    "\n",
    "            # verb_list.append(token)\n",
    "    # print(verb_list)\n",
    "\n",
    "    return count_verb\n",
    "\n",
    "\n",
    "def count_noun_spacy(doc):\n",
    "    \"\"\"\n",
    "    Given a Spacy doc object return counts of the \n",
    "    following parts-of-speech: nouns.\n",
    "    \n",
    "    Optional: uncomment the code lines to collect and \n",
    "    print a list of the tagged words.\n",
    "    \"\"\"\n",
    "    count_noun = 0\n",
    "    # noun_list = []\n",
    "\n",
    "    for token in doc:\n",
    "        if token.pos_ == 'NOUN':\n",
    "            count_noun += 1\n",
    "\n",
    "            # noun_list.append(token)\n",
    "    # print(noun_list)\n",
    "        \n",
    "    return count_noun\n",
    "\n",
    "\n",
    "def count_adj_spacy(doc):\n",
    "    \"\"\"\n",
    "    Given a Spacy doc object return counts of the \n",
    "    following parts-of-speech: adjectives.\n",
    "    \n",
    "    Optional: uncomment the code lines to collect and \n",
    "    print a list of the tagged words.\n",
    "    \"\"\"\n",
    "    count_adj = 0\n",
    "    # adj_list = []\n",
    "\n",
    "    for token in doc:\n",
    "        if token.pos_ == 'ADJ':\n",
    "            count_adj += 1\n",
    "\n",
    "            # adj_list.append(token)\n",
    "    # print(adj_list)\n",
    "        \n",
    "    return count_adj\n",
    "\n",
    "\n",
    "def count_nums_spacy(doc):\n",
    "    \"\"\"\n",
    "    Given a Spacy doc object return counts of the \n",
    "    following parts-of-speech: numbers.\n",
    "    \n",
    "    Optional: uncomment the code lines to collect and \n",
    "    print a list of the tagged words.\n",
    "    \"\"\" \n",
    "    count_nums = 0\n",
    "    # nums_list = []\n",
    "\n",
    "    for token in doc:\n",
    "        if token.pos_ == 'NUM':\n",
    "            count_nums += 1\n",
    "\n",
    "            # nums_list.append(token)\n",
    "    # print(nums_list)\n",
    "        \n",
    "    return count_nums\n",
    "\n",
    "\n",
    "def count_pron_spacy(doc):\n",
    "    \"\"\"\n",
    "    Given a Spacy doc object return counts of the \n",
    "    following parts-of-speech: pronouns.\n",
    "    \n",
    "    Optional: uncomment the code lines to collect and \n",
    "    print a list of the tagged words.\n",
    "    \"\"\"\n",
    "    count_pron = 0\n",
    "    # pron_list = []\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.pos_ == 'PRON':\n",
    "            count_pron += 1\n",
    "\n",
    "            # pron_list.append(token)\n",
    "    # print(pron_list)\n",
    "        \n",
    "    return count_pron\n",
    "\n",
    "\n",
    "def count_nnps_spacy(doc):\n",
    "    \"\"\"\n",
    "    Given a Spacy doc object return counts of the \n",
    "    following parts-of-speech: plural proper nouns.\n",
    "    \n",
    "    Optional: uncomment the code lines to collect and \n",
    "    print a list of the tagged words.\n",
    "    \"\"\"\n",
    "    count_nnps = 0\n",
    "    # nnps_list = []\n",
    "\n",
    "    for token in doc:\n",
    "        if token.tag_ == 'NNPS':\n",
    "            count_nnps += 1\n",
    "        \n",
    "    return count_nnps\n",
    "\n",
    "\n",
    "def count_vb_spacy(doc):\n",
    "    \"\"\"\n",
    "    Given a Spacy doc object return counts of the \n",
    "    following parts-of-speech: base form verbs.\n",
    "    \n",
    "    Optional: uncomment the code lines to collect and \n",
    "    print a list of the tagged words.\n",
    "    \"\"\"\n",
    "    count_vb = 0\n",
    "    # vb_list = []\n",
    "\n",
    "    for token in doc:\n",
    "        if token.tag_ == 'VB':\n",
    "            count_vb += 1\n",
    "\n",
    "            # vb_list.append(token)\n",
    "    # print(vb_list)\n",
    "\n",
    "    return count_vb\n",
    "\n",
    "\n",
    "def count_nn_spacy(doc):\n",
    "    \"\"\"\n",
    "    Given a Spacy doc object return counts of the \n",
    "    following parts-of-speech: singular or mass nouns.\n",
    "    \n",
    "    Optional: uncomment the code lines to collect and \n",
    "    print a list of the tagged words.\n",
    "    \"\"\"\n",
    "    count_nn = 0\n",
    "    # nn_list = []\n",
    "\n",
    "    for token in doc:\n",
    "        if token.tag_ == 'NN':\n",
    "            count_nn += 1\n",
    "\n",
    "            # nn_list.append(token)\n",
    "    # print(nn_list)\n",
    "        \n",
    "    return count_nn\n",
    "\n",
    "\n",
    "def count_jj_spacy(doc):\n",
    "    \"\"\"\n",
    "    Given a Spacy doc object return counts of the \n",
    "    following parts-of-speech: adjectives.\n",
    "    \n",
    "    Optional: uncomment the code lines to collect and \n",
    "    print a list of the tagged words.\n",
    "    \"\"\"\n",
    "    count_jj = 0\n",
    "    # jj_list = []\n",
    "\n",
    "    for token in doc:\n",
    "        if token.tag_ == 'JJ':\n",
    "            count_jj += 1\n",
    "\n",
    "            # jj_list.append(token)\n",
    "    # print(jj_list)\n",
    "        \n",
    "    return count_jj\n",
    "\n",
    "\n",
    "def count_cd_spacy(doc):\n",
    "    \"\"\"\n",
    "    Given a Spacy doc object return counts of the \n",
    "    following parts-of-speech: cardinal numbers.\n",
    "    \n",
    "    Optional: uncomment the code lines to collect and \n",
    "    print a list of the tagged words.\n",
    "    \"\"\" \n",
    "    count_cd = 0\n",
    "    # cd_list = []\n",
    "\n",
    "    for token in doc:\n",
    "        if token.tag_ == 'CD':\n",
    "            count_cd += 1\n",
    "\n",
    "            # nums_cd.append(token)\n",
    "    # print(cd_list)\n",
    "        \n",
    "    return count_cd\n",
    "\n",
    "\n",
    "def count_prp_spacy(doc):\n",
    "    \"\"\"\n",
    "    Given a Spacy doc object return counts of the \n",
    "    following parts-of-speech: personal pronouns.\n",
    "    \n",
    "    Optional: uncomment the code lines to collect and \n",
    "    print a list of the tagged words.\n",
    "    \"\"\"\n",
    "    count_prp = 0\n",
    "    # prp_list = []\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.tag_ == 'PRP':\n",
    "            count_prp += 1\n",
    "\n",
    "            # prp_list.append(token)\n",
    "    # print(prp_list)\n",
    "        \n",
    "    return count_prp\n",
    "\n",
    "\n",
    "def count_rb_spacy(doc):\n",
    "    \"\"\"\n",
    "    Given a Spacy doc object return counts of the \n",
    "    following parts-of-speech: adverbs.\n",
    "    \n",
    "    Optional: uncomment the code lines to collect and \n",
    "    print a list of the tagged words.\n",
    "    \"\"\"\n",
    "    count_rb = 0\n",
    "    # rb_list = []\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.tag_ == 'RB':\n",
    "            count_rb += 1\n",
    "\n",
    "            # rb_list.append(token)\n",
    "    # print(rb_list)\n",
    "        \n",
    "    return count_rb\n",
    "\n",
    "\n",
    "def count_cc_spacy(doc):\n",
    "    \"\"\"\n",
    "    Given a Spacy doc object return counts of the \n",
    "    following parts-of-speech: coordinating conjunctions.\n",
    "    \n",
    "    Optional: uncomment the code lines to collect and \n",
    "    print a list of the tagged words.\n",
    "    \"\"\"\n",
    "    count_cc = 0\n",
    "    # cc_list = []\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.tag_ == 'CC':\n",
    "            count_cc += 1\n",
    "\n",
    "            # cc_list.append(token)\n",
    "    # print(cc_list)\n",
    "        \n",
    "    return count_cc\n",
    "\n",
    "\n",
    "def count_nnp_spacy(doc):\n",
    "    \"\"\"\n",
    "    Given a Spacy doc object return counts of the \n",
    "    following parts-of-speech: singular proper nouns.\n",
    "    \n",
    "    Optional: uncomment the code lines to collect and \n",
    "    print a list of the tagged words.\n",
    "    \"\"\"\n",
    "    count_nnp = 0\n",
    "    # nnp_list = []\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.tag_ == 'NNP':\n",
    "            count_nnp += 1\n",
    "\n",
    "            # nnp_list.append(token)\n",
    "    # print(nnp_list)\n",
    "        \n",
    "    return count_nnp\n",
    "\n",
    "\n",
    "def count_vbd_spacy(doc):\n",
    "    \"\"\"\n",
    "    Given a Spacy doc object return counts of the \n",
    "    following parts-of-speech: past tense verbs.\n",
    "    \n",
    "    Optional: uncomment the code lines to collect and \n",
    "    print a list of the tagged words.\n",
    "    \"\"\"\n",
    "    count_vbd = 0\n",
    "    # vbd_list = []\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.tag_ == 'VBD':\n",
    "            count_vbd += 1\n",
    "\n",
    "            # vbd_list.append(token)\n",
    "    # print(vbd_list)\n",
    "        \n",
    "    return count_vbd\n",
    "\n",
    "\n",
    "def count_vbz_spacy(doc):\n",
    "    \"\"\"\n",
    "    Given a Spacy doc object return counts of the \n",
    "    following parts-of-speech: third-person singular present verbs.\n",
    "    \n",
    "    Optional: uncomment the code lines to collect and \n",
    "    print a list of the tagged words.\n",
    "    \"\"\"\n",
    "    count_vbz = 0\n",
    "    # vbz_list = []\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.tag_ == 'VBZ':\n",
    "            count_vbz += 1\n",
    "\n",
    "            # vbz_list.append(token)\n",
    "    # print(vbz_list)\n",
    "        \n",
    "    return count_vbz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_features_pipe(text_col, df):\n",
    "    \"\"\"\n",
    "    Process given text column of a dataframe to \n",
    "    extract linguistic features and add them to\n",
    "    the dataframe. Return the updated dataframe.\n",
    "    \"\"\"\n",
    "    \n",
    "    input_col = df[text_col]  \n",
    "    \n",
    "    propn_count = []\n",
    "    verb_count = []\n",
    "    noun_count = []\n",
    "    adj_count = []\n",
    "    nums_count = []\n",
    "    pron_count = []\n",
    "    \n",
    "    nnps_count = []\n",
    "    vb_count = []\n",
    "    nn_count = []\n",
    "    jj_count = []\n",
    "    cd_count = []\n",
    "    prp_count = []\n",
    "    rb_count = []\n",
    "    cc_count = []\n",
    "    nnp_count = []\n",
    "    vbd_count = []\n",
    "    vbz_count = []\n",
    "    \n",
    "    # spaCy processing pipeline\n",
    "    nlp_text_pipe = nlp.pipe(input_col, batch_size=20)\n",
    "    \n",
    "    for doc in nlp_text_pipe:\n",
    "        \n",
    "        # POS tags\n",
    "        # Universal POS Tags\n",
    "        # http://universaldependencies.org/u/pos/\n",
    "        \n",
    "        # Count proper nouns\n",
    "        propn_total = 0\n",
    "        count_propn = count_propn_spacy(doc)\n",
    "        propn_total += count_propn\n",
    "        propn_count.append(propn_total)\n",
    "        \n",
    "        # Count verbs\n",
    "        verb_total = 0\n",
    "        count_verb = count_verb_spacy(doc)\n",
    "        verb_total += count_verb\n",
    "        verb_count.append(verb_total)\n",
    "        \n",
    "        # Count nouns\n",
    "        noun_total = 0\n",
    "        count_noun = count_noun_spacy(doc)\n",
    "        noun_total += count_noun\n",
    "        noun_count.append(noun_total)\n",
    "        \n",
    "        # Count adjectives\n",
    "        adj_total = 0\n",
    "        count_adj = count_adj_spacy(doc)\n",
    "        adj_total += count_adj\n",
    "        adj_count.append(adj_total)\n",
    "        \n",
    "        # Count numbers\n",
    "        nums_total = 0\n",
    "        count_nums = count_nums_spacy(doc)\n",
    "        nums_total += count_nums\n",
    "        nums_count.append(nums_total)\n",
    "        \n",
    "        # Count pronouns\n",
    "        pron_total = 0\n",
    "        count_pron = count_pron_spacy(doc)\n",
    "        pron_total += count_pron\n",
    "        pron_count.append(pron_total)\n",
    "        \n",
    "        # POS tags (English)\n",
    "        # OntoNotes 5 / Penn Treebank\n",
    "        # https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "        \n",
    "        # Count plural proper nouns\n",
    "        nnps_total = 0\n",
    "        count_nnps = count_nnps_spacy(doc)\n",
    "        nnps_total += count_nnps\n",
    "        nnps_count.append(nnps_total)\n",
    "        \n",
    "        # Count base form verbs\n",
    "        vb_total = 0\n",
    "        count_vb = count_vb_spacy(doc)\n",
    "        vb_total += count_vb\n",
    "        vb_count.append(vb_total)\n",
    "        \n",
    "        # Count singular or mass nouns\n",
    "        nn_total = 0\n",
    "        count_nn = count_nn_spacy(doc)\n",
    "        nn_total += count_nn\n",
    "        nn_count.append(nn_total)\n",
    "        \n",
    "        # Count adjectives\n",
    "        jj_total = 0\n",
    "        count_jj = count_jj_spacy(doc)\n",
    "        jj_total += count_jj\n",
    "        jj_count.append(jj_total)\n",
    "        \n",
    "        # Count cardinal numbers\n",
    "        cd_total = 0\n",
    "        count_cd = count_cd_spacy(doc)\n",
    "        cd_total += count_cd\n",
    "        cd_count.append(cd_total)\n",
    "        \n",
    "        # Count personal pronouns\n",
    "        prp_total = 0\n",
    "        count_prp = count_prp_spacy(doc)\n",
    "        prp_total += count_prp\n",
    "        prp_count.append(prp_total)\n",
    "        \n",
    "        # Count adverbs\n",
    "        rb_total = 0\n",
    "        count_rb = count_rb_spacy(doc)\n",
    "        rb_total += count_rb\n",
    "        rb_count.append(rb_total)\n",
    "        \n",
    "        # Count coordinating conjunctions\n",
    "        cc_total = 0\n",
    "        count_cc = count_cc_spacy(doc)\n",
    "        cc_total += count_cc\n",
    "        cc_count.append(cc_total)\n",
    "        \n",
    "        # Count singular proper nouns\n",
    "        nnp_total = 0\n",
    "        count_nnp = count_nnp_spacy(doc)\n",
    "        nnp_total += count_nnp\n",
    "        nnp_count.append(nnp_total)\n",
    "        \n",
    "        # Count past tense verbs\n",
    "        vbd_total = 0\n",
    "        count_vbd = count_vbd_spacy(doc)\n",
    "        vbd_total += count_vbd\n",
    "        vbd_count.append(vbd_total)\n",
    "        \n",
    "        # Count third-person singular present verbs\n",
    "        vbz_total = 0\n",
    "        count_vbz = count_vbz_spacy(doc)\n",
    "        vbz_total += count_vbz\n",
    "        vbz_count.append(vbz_total)\n",
    "        \n",
    "    # Add features using the textstat library to the dataframe\n",
    "    # https://pypi.org/project/textstat/\n",
    "    df['word_count'] = input_col.apply(lambda x: textstat.lexicon_count(x, removepunct=True)) \n",
    "    df['syll_count'] = input_col.apply(lambda x: textstat.syllable_count(x))\n",
    "    df['polysyll_count'] = input_col.apply(lambda x: textstat.polysyllabcount(x)) # Returns the number of words with a syllable count greater than or equal to 3.\n",
    "    df['monosyll_count'] = input_col.apply(lambda x: textstat.monosyllabcount(x)) # Returns the number of words with a syllable count equal to one.\n",
    "    \n",
    "    # Add features using the textfeatures library to the dataframe\n",
    "    # https://towardsdatascience.com/textfeatures-library-for-extracting-basic-features-from-text-data-f98ba90e3932\n",
    "    tf.stopwords_count(df,text_col,'stopwords_count')\n",
    "    # tf.stopwords(df,text_col,'stopwords')  # Include a column that lists the stopwords found in the text\n",
    "    \n",
    "    try:\n",
    "        tf.avg_word_length(df,text_col,'avg_word_length')\n",
    "    except:\n",
    "        df['avg_word_length'] = 0\n",
    "    \n",
    "    try:\n",
    "        tf.char_count(df,text_col,'char_count')\n",
    "    except:\n",
    "        df['char_count'] = 0\n",
    "    \n",
    "    # Add features based on the spaCy pipeline to the dataframe\n",
    "    df['propn_count'] = propn_count\n",
    "    df['verb_count'] = verb_count\n",
    "    df['noun_count'] = noun_count\n",
    "    df['adj_count'] = adj_count\n",
    "    df['nums_count'] = nums_count\n",
    "    df['pron_count'] = pron_count\n",
    "    \n",
    "    df['nnps_count'] = nnps_count\n",
    "    df['vb_count'] = vb_count\n",
    "    df['nn_count'] = nn_count\n",
    "    df['jj_count'] = jj_count\n",
    "    df['cd_count'] = cd_count\n",
    "    df['prp_count'] = prp_count\n",
    "    df['rb_count'] = rb_count\n",
    "    df['cc_count'] = cc_count\n",
    "    df['nnp_count'] = nnp_count\n",
    "    df['vbd_count'] = vbd_count\n",
    "    df['vbz_count'] = vbz_count\n",
    "    \n",
    "    # Add frequency columns\n",
    "    \n",
    "    df['propn_freq'] = df['propn_count']/df['word_count']\n",
    "    df['verb_freq'] = df['verb_count']/df['word_count']\n",
    "    df['noun_freq'] = df['noun_count']/df['word_count']\n",
    "    df['adj_freq'] = df['adj_count']/df['word_count']\n",
    "    df['nums_freq'] = df['nums_count']/df['word_count']\n",
    "    df['pron_freq'] = df['pron_count']/df['word_count']\n",
    "    \n",
    "    df['nnps_freq'] = df['nnps_count']/df['word_count']\n",
    "    df['vb_freq'] = df['vb_count']/df['word_count']\n",
    "    df['nn_freq'] = df['nn_count']/df['word_count']\n",
    "    df['jj_freq'] = df['jj_count']/df['word_count']\n",
    "    df['cd_freq'] = df['cd_count']/df['word_count']\n",
    "    df['prp_freq'] = df['prp_count']/df['word_count']\n",
    "    df['rb_freq'] = df['rb_count']/df['word_count']\n",
    "    df['cc_freq'] = df['cc_count']/df['word_count']\n",
    "    df['nnp_freq'] = df['nnp_count']/df['word_count']\n",
    "    df['vbd_freq'] = df['vbd_count']/df['word_count']\n",
    "    df['vbz_freq'] = df['vbz_count']/df['word_count']\n",
    "    \n",
    "    df['polysyll_freq'] = df['polysyll_count']/df['word_count']\n",
    "    df['monosyll_freq'] = df['monosyll_count']/df['word_count']\n",
    "    df['stopword_freq'] = df['stopwords_count']/df['word_count']\n",
    "    \n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_col = 'clean_text'  # The name of dataframe column containing the text to be processed\n",
    "features_df = text_features_pipe(text_col, clean_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5 # top n TF-IDF words\n",
    "\n",
    "tfidf = TfidfVectorizer(token_pattern=r\"\\w+\") # no words are left out\n",
    "X = tfidf.fit_transform(features_df['clean_text'])\n",
    "ind = (-X.todense()).argpartition(n)[:, :n]\n",
    "top_words = pd.Series(\n",
    "    map(\n",
    "        lambda words_values: dict(zip(*words_values)),\n",
    "        zip(\n",
    "            np.array(tfidf.get_feature_names_out())[ind],\n",
    "            np.asarray(np.take_along_axis(X, ind, axis=1).todense()),\n",
    "        ),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words_df = pd.DataFrame({'tf_idf':top_words.values})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = pd.concat([features_df, top_words_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df[\"tf_idf_sum\"] = features_df[\"tf_idf\"].apply(lambda x : sum(x.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df.to_pickle(f\"{export_filename}_FEATURES.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <h3>Reload saved dataframe (if required)</h3>\n",
    "    <p>The previously saved dataframe of articles including all features can be reloaded by uncommenting and running the following cell.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features_df = pd.read_pickle(f\"{export_filename}_FEATURES.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply the saved model to the final dataset and export a csv file of results\n",
    "\n",
    "The following cells remove any rows with missing values, apply the saved model to the dataset, and display and export the final results in a dataframe sorted by confidence (proximity to decision boundary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = features_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df.reset_index(drop=True, inplace=True) # reset the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>newspaper_id</th>\n",
       "      <th>newspaper</th>\n",
       "      <th>article_id</th>\n",
       "      <th>avg_line_width</th>\n",
       "      <th>min_line_width</th>\n",
       "      <th>max_line_width</th>\n",
       "      <th>line_width_range</th>\n",
       "      <th>avg_line_offset</th>\n",
       "      <th>max_line_offset</th>\n",
       "      <th>...</th>\n",
       "      <th>rb_freq</th>\n",
       "      <th>cc_freq</th>\n",
       "      <th>nnp_freq</th>\n",
       "      <th>vbd_freq</th>\n",
       "      <th>vbz_freq</th>\n",
       "      <th>polysyll_freq</th>\n",
       "      <th>monosyll_freq</th>\n",
       "      <th>stopword_freq</th>\n",
       "      <th>tf_idf</th>\n",
       "      <th>tf_idf_sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1889-04-09</td>\n",
       "      <td>TEML</td>\n",
       "      <td>Temuka Leader</td>\n",
       "      <td>1</td>\n",
       "      <td>434.083871</td>\n",
       "      <td>74.0</td>\n",
       "      <td>517.0</td>\n",
       "      <td>443.0</td>\n",
       "      <td>35.425806</td>\n",
       "      <td>386.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013970</td>\n",
       "      <td>0.024447</td>\n",
       "      <td>0.229336</td>\n",
       "      <td>0.027939</td>\n",
       "      <td>0.038417</td>\n",
       "      <td>0.091967</td>\n",
       "      <td>0.707800</td>\n",
       "      <td>0.373690</td>\n",
       "      <td>{'april': 0.332146230160219, 'the': 0.45285429...</td>\n",
       "      <td>1.304603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1889-04-09</td>\n",
       "      <td>TEML</td>\n",
       "      <td>Temuka Leader</td>\n",
       "      <td>2</td>\n",
       "      <td>442.789474</td>\n",
       "      <td>97.0</td>\n",
       "      <td>514.0</td>\n",
       "      <td>417.0</td>\n",
       "      <td>26.894737</td>\n",
       "      <td>385.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008772</td>\n",
       "      <td>0.017544</td>\n",
       "      <td>0.254386</td>\n",
       "      <td>0.017544</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.122807</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>0.350877</td>\n",
       "      <td>{'commons': 0.25102992680976377, 'the': 0.2592...</td>\n",
       "      <td>1.094207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1889-04-09</td>\n",
       "      <td>TEML</td>\n",
       "      <td>Temuka Leader</td>\n",
       "      <td>3</td>\n",
       "      <td>415.200000</td>\n",
       "      <td>354.0</td>\n",
       "      <td>480.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>38.200000</td>\n",
       "      <td>123.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.269231</td>\n",
       "      <td>{'deaths': 0.44817071134752623, 'yellow': 0.41...</td>\n",
       "      <td>1.800691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1889-04-09</td>\n",
       "      <td>TEML</td>\n",
       "      <td>Temuka Leader</td>\n",
       "      <td>4</td>\n",
       "      <td>446.551724</td>\n",
       "      <td>93.0</td>\n",
       "      <td>517.0</td>\n",
       "      <td>424.0</td>\n",
       "      <td>42.017241</td>\n",
       "      <td>391.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023256</td>\n",
       "      <td>0.014535</td>\n",
       "      <td>0.142442</td>\n",
       "      <td>0.031977</td>\n",
       "      <td>0.031977</td>\n",
       "      <td>0.090116</td>\n",
       "      <td>0.729651</td>\n",
       "      <td>0.456395</td>\n",
       "      <td>{'samoa': 0.28509833166048637, 'calliope': 0.3...</td>\n",
       "      <td>1.589073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1889-04-09</td>\n",
       "      <td>TEML</td>\n",
       "      <td>Temuka Leader</td>\n",
       "      <td>5</td>\n",
       "      <td>475.102564</td>\n",
       "      <td>82.0</td>\n",
       "      <td>515.0</td>\n",
       "      <td>433.0</td>\n",
       "      <td>11.910256</td>\n",
       "      <td>249.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.039014</td>\n",
       "      <td>0.022587</td>\n",
       "      <td>0.145791</td>\n",
       "      <td>0.032854</td>\n",
       "      <td>0.024641</td>\n",
       "      <td>0.108830</td>\n",
       "      <td>0.655031</td>\n",
       "      <td>0.410678</td>\n",
       "      <td>{'the': 0.3726277068134216, 'parnell': 0.23936...</td>\n",
       "      <td>1.254078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>1888-01-19</td>\n",
       "      <td>PATM</td>\n",
       "      <td>Patea Mail</td>\n",
       "      <td>18</td>\n",
       "      <td>573.275862</td>\n",
       "      <td>211.0</td>\n",
       "      <td>631.0</td>\n",
       "      <td>420.0</td>\n",
       "      <td>50.482759</td>\n",
       "      <td>90.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043590</td>\n",
       "      <td>0.023077</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.069231</td>\n",
       "      <td>0.005128</td>\n",
       "      <td>0.064103</td>\n",
       "      <td>0.758974</td>\n",
       "      <td>0.435897</td>\n",
       "      <td>{'a': 0.3928149946200692, 'the': 0.31073259159...</td>\n",
       "      <td>1.242418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>1888-01-19</td>\n",
       "      <td>PATM</td>\n",
       "      <td>Patea Mail</td>\n",
       "      <td>19</td>\n",
       "      <td>563.437500</td>\n",
       "      <td>21.0</td>\n",
       "      <td>631.0</td>\n",
       "      <td>610.0</td>\n",
       "      <td>41.776786</td>\n",
       "      <td>110.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029647</td>\n",
       "      <td>0.047891</td>\n",
       "      <td>0.078677</td>\n",
       "      <td>0.064994</td>\n",
       "      <td>0.010262</td>\n",
       "      <td>0.078677</td>\n",
       "      <td>0.758267</td>\n",
       "      <td>0.469783</td>\n",
       "      <td>{'he': 0.24825631384343724, 'the': 0.349620658...</td>\n",
       "      <td>1.248073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>1888-01-19</td>\n",
       "      <td>PATM</td>\n",
       "      <td>Patea Mail</td>\n",
       "      <td>20</td>\n",
       "      <td>500.547486</td>\n",
       "      <td>62.0</td>\n",
       "      <td>583.0</td>\n",
       "      <td>521.0</td>\n",
       "      <td>19.122905</td>\n",
       "      <td>165.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.048801</td>\n",
       "      <td>0.030604</td>\n",
       "      <td>0.165426</td>\n",
       "      <td>0.081059</td>\n",
       "      <td>0.013234</td>\n",
       "      <td>0.067825</td>\n",
       "      <td>0.784119</td>\n",
       "      <td>0.353184</td>\n",
       "      <td>{'knighton': 0.33324062246197195, 'the': 0.246...</td>\n",
       "      <td>1.227557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>1888-01-19</td>\n",
       "      <td>PATM</td>\n",
       "      <td>Patea Mail</td>\n",
       "      <td>21</td>\n",
       "      <td>480.777778</td>\n",
       "      <td>52.0</td>\n",
       "      <td>577.0</td>\n",
       "      <td>525.0</td>\n",
       "      <td>30.811966</td>\n",
       "      <td>376.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026027</td>\n",
       "      <td>0.010959</td>\n",
       "      <td>0.235616</td>\n",
       "      <td>0.031507</td>\n",
       "      <td>0.021918</td>\n",
       "      <td>0.115068</td>\n",
       "      <td>0.726027</td>\n",
       "      <td>0.316438</td>\n",
       "      <td>{'january': 0.24596206600351617, 'the': 0.2957...</td>\n",
       "      <td>0.989050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>1888-01-19</td>\n",
       "      <td>PATM</td>\n",
       "      <td>Patea Mail</td>\n",
       "      <td>22</td>\n",
       "      <td>520.266667</td>\n",
       "      <td>104.0</td>\n",
       "      <td>582.0</td>\n",
       "      <td>478.0</td>\n",
       "      <td>35.777778</td>\n",
       "      <td>274.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013928</td>\n",
       "      <td>0.022284</td>\n",
       "      <td>0.225627</td>\n",
       "      <td>0.033426</td>\n",
       "      <td>0.019499</td>\n",
       "      <td>0.025070</td>\n",
       "      <td>0.866295</td>\n",
       "      <td>0.281337</td>\n",
       "      <td>{'r': 0.2080155950571782, 'i': 0.1629699679699...</td>\n",
       "      <td>0.832558</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>416 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          date newspaper_id      newspaper  article_id  avg_line_width  \\\n",
       "0   1889-04-09         TEML  Temuka Leader           1      434.083871   \n",
       "1   1889-04-09         TEML  Temuka Leader           2      442.789474   \n",
       "2   1889-04-09         TEML  Temuka Leader           3      415.200000   \n",
       "3   1889-04-09         TEML  Temuka Leader           4      446.551724   \n",
       "4   1889-04-09         TEML  Temuka Leader           5      475.102564   \n",
       "..         ...          ...            ...         ...             ...   \n",
       "411 1888-01-19         PATM     Patea Mail          18      573.275862   \n",
       "412 1888-01-19         PATM     Patea Mail          19      563.437500   \n",
       "413 1888-01-19         PATM     Patea Mail          20      500.547486   \n",
       "414 1888-01-19         PATM     Patea Mail          21      480.777778   \n",
       "415 1888-01-19         PATM     Patea Mail          22      520.266667   \n",
       "\n",
       "     min_line_width  max_line_width  line_width_range  avg_line_offset  \\\n",
       "0              74.0           517.0             443.0        35.425806   \n",
       "1              97.0           514.0             417.0        26.894737   \n",
       "2             354.0           480.0             126.0        38.200000   \n",
       "3              93.0           517.0             424.0        42.017241   \n",
       "4              82.0           515.0             433.0        11.910256   \n",
       "..              ...             ...               ...              ...   \n",
       "411           211.0           631.0             420.0        50.482759   \n",
       "412            21.0           631.0             610.0        41.776786   \n",
       "413            62.0           583.0             521.0        19.122905   \n",
       "414            52.0           577.0             525.0        30.811966   \n",
       "415           104.0           582.0             478.0        35.777778   \n",
       "\n",
       "     max_line_offset  ...   rb_freq   cc_freq  nnp_freq  vbd_freq  vbz_freq  \\\n",
       "0              386.0  ...  0.013970  0.024447  0.229336  0.027939  0.038417   \n",
       "1              385.0  ...  0.008772  0.017544  0.254386  0.017544  0.000000   \n",
       "2              123.0  ...  0.076923  0.038462  0.153846  0.000000  0.000000   \n",
       "3              391.0  ...  0.023256  0.014535  0.142442  0.031977  0.031977   \n",
       "4              249.0  ...  0.039014  0.022587  0.145791  0.032854  0.024641   \n",
       "..               ...  ...       ...       ...       ...       ...       ...   \n",
       "411             90.0  ...  0.043590  0.023077  0.038462  0.069231  0.005128   \n",
       "412            110.0  ...  0.029647  0.047891  0.078677  0.064994  0.010262   \n",
       "413            165.0  ...  0.048801  0.030604  0.165426  0.081059  0.013234   \n",
       "414            376.0  ...  0.026027  0.010959  0.235616  0.031507  0.021918   \n",
       "415            274.0  ...  0.013928  0.022284  0.225627  0.033426  0.019499   \n",
       "\n",
       "     polysyll_freq  monosyll_freq  stopword_freq  \\\n",
       "0         0.091967       0.707800       0.373690   \n",
       "1         0.122807       0.684211       0.350877   \n",
       "2         0.038462       0.692308       0.269231   \n",
       "3         0.090116       0.729651       0.456395   \n",
       "4         0.108830       0.655031       0.410678   \n",
       "..             ...            ...            ...   \n",
       "411       0.064103       0.758974       0.435897   \n",
       "412       0.078677       0.758267       0.469783   \n",
       "413       0.067825       0.784119       0.353184   \n",
       "414       0.115068       0.726027       0.316438   \n",
       "415       0.025070       0.866295       0.281337   \n",
       "\n",
       "                                                tf_idf  tf_idf_sum  \n",
       "0    {'april': 0.332146230160219, 'the': 0.45285429...    1.304603  \n",
       "1    {'commons': 0.25102992680976377, 'the': 0.2592...    1.094207  \n",
       "2    {'deaths': 0.44817071134752623, 'yellow': 0.41...    1.800691  \n",
       "3    {'samoa': 0.28509833166048637, 'calliope': 0.3...    1.589073  \n",
       "4    {'the': 0.3726277068134216, 'parnell': 0.23936...    1.254078  \n",
       "..                                                 ...         ...  \n",
       "411  {'a': 0.3928149946200692, 'the': 0.31073259159...    1.242418  \n",
       "412  {'he': 0.24825631384343724, 'the': 0.349620658...    1.248073  \n",
       "413  {'knighton': 0.33324062246197195, 'the': 0.246...    1.227557  \n",
       "414  {'january': 0.24596206600351617, 'the': 0.2957...    0.989050  \n",
       "415  {'r': 0.2080155950571782, 'i': 0.1629699679699...    0.832558  \n",
       "\n",
       "[416 rows x 61 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genres_binary(df, model):\n",
    "    \"\"\"\n",
    "    Run the model, and return the dataframe\n",
    "    with appended predictions.\n",
    "    \"\"\"\n",
    "\n",
    "    X = df.filter(features, axis=1)\n",
    "    indices = df.index.values\n",
    "\n",
    "    y_pred = model.predict(X)\n",
    "    y_conf = model.decision_function(X)\n",
    "    \n",
    "\n",
    "    # Add the predictions to a copy of the original dataframe\n",
    "    df_new = df.copy()\n",
    "    df_new.loc[indices,'pred'] = y_pred\n",
    "    df_new.loc[indices,'conf'] = y_conf\n",
    "\n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model from disk\n",
    "loaded_model = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_df = genres_binary(features_df, loaded_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = preds_df.filter([\"date\", \"newspaper_id\", \"newspaper\", \"article_id\", \"title\", \"text\", \"pred\", \"conf\"], axis=1)\n",
    "final_df[\"newspaper_web\"] = final_df[\"date\"].astype('string')\n",
    "final_df[\"newspaper_web\"] = final_df[\"newspaper_web\"].str.replace('-','/')\n",
    "final_df[\"newspaper_web\"] = \"https://paperspast.natlib.govt.nz/newspapers/\" \\\n",
    "                            + final_df[\"newspaper\"].str.replace(' ','-', regex = False).str.lower().str.replace(\"'\", \"\", regex = False).str.replace(\".\", \"\", regex = False) \\\n",
    "                            + \"/\" \\\n",
    "                            + final_df[\"newspaper_web\"]\n",
    "\n",
    "# This line of code will provide a dataframe with only the articles classified as the given genre\n",
    "# final_df = final_df[final_df[\"pred\"] == 1].sort_values(by=\"conf\", ascending=False)\n",
    "\n",
    "# This line of code will provide a dataframe with all articles - ranked by distance from the decision boundary for the given genre\n",
    "final_df = final_df.sort_values(by=\"conf\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the line below to view the final dataframe if required\n",
    "\n",
    "# display(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the dataframe of results to a CSV file \n",
    "final_df.to_csv(f\"{export_filename}_FINAL.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
