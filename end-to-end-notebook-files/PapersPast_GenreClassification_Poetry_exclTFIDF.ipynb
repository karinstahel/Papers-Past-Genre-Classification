{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Papers Past Genre Classification\n",
    "# End-to-End Notebook: Poetry\n",
    "---\n",
    "\n",
    "## Logistic Regression (excluding TF-IDF features)\n",
    "\n",
    "* This notebook processes the raw Papers Past METS/ALTO XML files (saved in tar.gz format by newspaper and year) to return a Pandas dataframe of articles sorted by the probability that they are poetry. \n",
    "* The final dataframe is exported as a csv file with links to the online Papers Past newspaper issue for each article. This allows you to view the scanned image of the original article.   \n",
    "* The given number of newspaper issues are randomly selected, with the option to set a seed for reproducibility. \n",
    "* This version of the notebook excludes TF-IDF feature extraction, which is computationally expensive and may result in out of memory errors for larger sample sizes. \n",
    "* The model is less precise than the version that incorporates the TF-IDF feature and consequently more false positives can be expected in the results. However, this model still performs well in terms of recall, meaning you can expect most of the poetry in the sample to be returned high up in the final dataframe of results when sorted by probability.\n",
    "* Note that the sampling and feature extraction processes can take quite a while to run. For example, a sample of 300 newspaper issues could take up to an hour on an average laptop and 6,000 issues (about 2% of the entire dataset) could take 20+ hours. You should change your computer's settings to prevent it going to sleep or powering off while the notebook is running.  \n",
    "* Dataframes are saved in pickle format following sampling and feature extraction so that if an error occurs or the notebook doesn't run all the way through for some reason, the saved dataframe can be loaded and you can restart the process from that point. These points in the notebook are highlighted in orange. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Owner\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "import re\n",
    "import os\n",
    "import statistics\n",
    "import tarfile\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "import pickle\n",
    "import time\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "\n",
    "import spacy\n",
    "import math\n",
    "import textstat\n",
    "import textfeatures as tf\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# Features\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Classifiers\n",
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell loads the mets namespace- there is no need to change anything\n",
    "# http://www.loc.gov/standards/mets/namespace.html\n",
    "\n",
    "NS = {'mets':'http://www.loc.gov/METS/'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <h3>Set the variables in the cell below before running all cells</h3>\n",
    "    <p>\n",
    "        <li>Select a random seed for reproducibility.\n",
    "        <li>Provide the filepath for the newspaper-year tar.gz files.\n",
    "        <li>Select the number of issues to sample.\n",
    "        <li>Provide a filename for saving dataframes and exporting the final csv file\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter a random seed for reproducibility of results\n",
    "random.seed(a=6)\n",
    "\n",
    "# Set the top level directory to sample from\n",
    "dir_name = 'E:/PapersPast_OpenData'\n",
    "\n",
    "# Set the number of newspaper issues to be sampled\n",
    "num_issues = 6000\n",
    "\n",
    "# Filename for saved files \n",
    "# Do not include filetype suffix such as .csv - this is added as part of the code\n",
    "export_filename = \"20220219_NewPoetry_6000issues_seed6_df_exclTFIDF\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the saved model and feature set\n",
    "\n",
    "* The following cells load the saved model and the relevant feature set. \n",
    "* Make sure that the model file is saved in the same location as this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filename of the saved scikit-learn model (should not need to be changed)\n",
    "filename = 'pp_logistic_reg_poetry_noTFIDF.sav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature set (should not need to be changed)\n",
    "# 'all_features_excl_penn' set from binary trials notebook (excl TFIDF)\n",
    "features = [\"propn_freq\", \n",
    "            \"verb_freq\", \n",
    "            \"noun_freq\", \n",
    "            \"adj_freq\",\n",
    "            \"nums_freq\", \n",
    "            \"pron_freq\", \n",
    "            \"stopword_freq\", \n",
    "            \"avg_line_offset\", \n",
    "            \"max_line_offset\", \n",
    "            \"avg_line_width\", \n",
    "            \"min_line_width\", \n",
    "            \"max_line_width\", \n",
    "            \"line_width_range\", \n",
    "            \"polysyll_freq\", \n",
    "            \"monosyll_freq\", \n",
    "            \"sentence_count\", \n",
    "            \"word_count\", \n",
    "            \"avg_word_length\", \n",
    "            \"char_count\"\n",
    "           ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data extraction and page layout features\n",
    "\n",
    "The following cells extract the article details and page layout features from the tar.gz files and return a Pandas dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tarball(filepath):\n",
    "    \"\"\"\n",
    "    Given path to tarball, open and return dictionary containing \n",
    "    article codes as keys and texts (as list of strings for each block) as values.\n",
    "    \"\"\"\n",
    "    newspaper_year = tarfile.open(filepath, mode='r')\n",
    "\n",
    "    # Next, return the members of the archive as a list of TarInfo objects. \n",
    "    # The list has the same order as the members in the archive.\n",
    "    # https://docs.python.org/3/library/tarfile.html\n",
    "    files = newspaper_year.getmembers() \n",
    "\n",
    "    issues = collect_issues(files)\n",
    "    selected_issue = select_random_issue(issues)\n",
    "    mets_tarinfo = issues[selected_issue][-1]\n",
    "    pages_tarinfo = issues[selected_issue][0:-1]\n",
    "    article_codes = mets2codes(mets_tarinfo, newspaper_year)\n",
    "    articles = codes2texts(article_codes, pages_tarinfo, newspaper_year, selected_issue)\n",
    "\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_issues(files):\n",
    "    \"\"\"\n",
    "    Given list of files in tarball, return a dictionary keyed\n",
    "    by the issue code with list of xml files of form [0001.xml, ..., mets.xml]\n",
    "    as values.\n",
    "    \"\"\"\n",
    "    issues = {}\n",
    "    issue_code = ''\n",
    "\n",
    "    for file in files:\n",
    "        match = re.search(\"[A-Z]*_\\d{8}$\", file.name)\n",
    "        if match:\n",
    "            issue_code = match.group(0)\n",
    "        if file.name.endswith('.xml'):\n",
    "            xml_list = issues.get(issue_code, [])\n",
    "            xml_list.append(file)\n",
    "            issues[issue_code] = xml_list\n",
    "\n",
    "    return issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_random_issue(issues):\n",
    "    \"\"\"\n",
    "    Select a random issue from a given dictionary of the all of a newspaper's issues for one year \n",
    "    (with the issue code as the key and the list of XML files as the elements).\n",
    "    \"\"\"\n",
    "    selected_issue = random.choice(list(issues))\n",
    "\n",
    "    return selected_issue "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mets2codes(mets_tarinfo, newspaper_year):\n",
    "    \"\"\"\n",
    "    Given mets as tarinfo, return text block codes for articles\n",
    "    contained in mets file. Edited for processing with tarfile\n",
    "    object newspaper_year.\n",
    "\n",
    "    Returns dictionary of article codes as keys,\n",
    "    with a 2-tuple containing the article title\n",
    "    and a list of corresponding text block codes as values.\n",
    "    \"\"\"\n",
    "    with newspaper_year.extractfile(mets_tarinfo) as file:\n",
    "        text = file.read()\n",
    "\n",
    "    art_dict = mets2codes_inner(text, newspaper_year)\n",
    "\n",
    "    return art_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mets2codes_inner(text, newspaper_year):\n",
    "    \"\"\"\n",
    "    Given METS file as text string, return a dictionary of\n",
    "    articles, with article codes as keys and, as values, tuples containing\n",
    "    the corresponding article title and a list of text blocks from the\n",
    "    corresponding ALTO files as values.\n",
    "    \"\"\"\n",
    "    mets_root = ET.fromstring(text) # Loads the mets xml file (which comes into the function as a string)\n",
    "    logical_structure = mets_root.find(\"./mets:structMap[@LABEL='Logical Structure']\", NS) # Finds the \"logical structure\" part of the file, which lists all the articles and the blocks they contain.\n",
    "    articles_div = logical_structure.findall(\".//mets:div[@TYPE='ARTICLE']\", NS) # This returns all of the \"div\" elements in the logical structure part of the xml that have the attribute \"TYPE='ARTICLE'\". This is where we lose the advertisements.\n",
    "\n",
    "    art_dict = {} # This is an empty dictionary which will collect what we need from the mets file. It will have articles ids as keys and have the ids of the text blocks which are part of the article as values.\n",
    "    for article in articles_div:\n",
    "        \n",
    "        attributes = article.attrib\n",
    "        article_id = attributes['DMDID']\n",
    "        article_title = attributes.get('LABEL', 'UNTITLED')\n",
    "\n",
    "        text_blocks = article.findall(\".//mets:div[@TYPE='TEXT']\", NS)\n",
    "        block_ids = []\n",
    "        for block in text_blocks:\n",
    "            try:\n",
    "                areas = block.findall(\".//mets:area\", NS)\n",
    "                for area in areas:\n",
    "                    block_id = area.attrib['BEGIN']\n",
    "                    block_ids.append(block_id)\n",
    "            except AttributeError:\n",
    "                print(f'Error in {newspaper_year}')\n",
    "        \n",
    "        art_dict[article_id] = (article_title, block_ids)\n",
    "\n",
    "    mets_root.clear() # When processing lots of these, we want to free up memory.\n",
    "\n",
    "    # print(art_dict)\n",
    "    return art_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def codes2texts(article_codes, pages_tarinfo, newspaper_year, selected_issue):\n",
    "    \"\"\"\n",
    "    Given article codes, the issue pages as tar info objects, \n",
    "    the newspaper year and the issue code, return a dictionary\n",
    "    with article codes as keys and a list of text blocks as\n",
    "    strings as values.\n",
    "    \"\"\"\n",
    "    page_roots = parse_pages_tar(pages_tarinfo, newspaper_year)\n",
    "    # page_roots returns a dictionary with pages numbers (of form 'P1'\n",
    "    # etc...) as keys and the XML roots of the pages as values.\n",
    "\n",
    "    texts_dict = codes2texts_inner(article_codes, page_roots, selected_issue)\n",
    "\n",
    "    # Clear roots.\n",
    "    for i in range(len(page_roots)):\n",
    "        k, v = page_roots.popitem()\n",
    "        v.clear()\n",
    "\n",
    "    return texts_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_pages_tar(pages, newspaper_year):\n",
    "    \"\"\"\n",
    "    Given iterable of paths to page files, return\n",
    "    dictionary with 'P1', 'P2', etc as keys, and the\n",
    "    root element of each page as values.\n",
    "    \"\"\"\n",
    "    page_roots = {}\n",
    "    for i, page in enumerate(pages):\n",
    "        with newspaper_year.extractfile(page) as f:\n",
    "            text = f.read()\n",
    "        root = ET.fromstring(text)\n",
    "        page_roots[f'P{i+1}'] = root\n",
    "\n",
    "    return page_roots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def codes2texts_inner(article_codes, page_roots, selected_issue):\n",
    "    \"\"\"\n",
    "    Given XML roots of ALTO pages and collection of article codes\n",
    "    and corresponding blocks, return a dictionary with article codes\n",
    "    as keys and a list of text blocks as strings as values.\n",
    "    \"\"\"\n",
    "    texts_dict = {}  \n",
    "    \n",
    "    for article_id in article_codes.keys():\n",
    "        title, blocks = article_codes[article_id]\n",
    "        text = []\n",
    "        line_widths = []\n",
    "        line_hpos = []\n",
    "        line_offsets = []\n",
    "        \n",
    "        for block in blocks:\n",
    "\n",
    "            # The block ids have page numbers as part. We collect the page number.\n",
    "            end_loc = block.find('_')\n",
    "            page_no = block[0:end_loc]\n",
    "\n",
    "            # Collect the relevant page (the alto file) for the block.\n",
    "            page_root = page_roots[page_no]\n",
    "\n",
    "            # Collect the specific block from the page and identify the desired elements \n",
    "            # (strings, lines, horizontal position etc.)\n",
    "            xml_block = page_root.find(f\".//TextBlock[@ID='{block}']\")\n",
    "\n",
    "            block_strings = xml_block.findall('.//String')\n",
    "            block_lines = xml_block.findall('.//TextLine')\n",
    "            block_hpos = int(xml_block.get(\"HPOS\"))\n",
    "\n",
    "            # Collect the information we want from the blocks.\n",
    "            block_as_string = process_block(block_strings)\n",
    "            block_line_widths = process_block_lines(block_lines)\n",
    "            block_line_hpos = process_lines_hpos(block_lines)\n",
    "        \n",
    "            block_line_offsets = [hpos - block_hpos for hpos in block_line_hpos]\n",
    "            \n",
    "            text.append(block_as_string)\n",
    "            line_widths.extend(block_line_widths)\n",
    "            line_hpos.extend(block_line_hpos)\n",
    "            line_offsets.extend(block_line_offsets)\n",
    "\n",
    "        text = ' '.join(text)\n",
    "        issue_article_id = selected_issue + '_' + article_id[7:]\n",
    "        # texts_dict[issue_article_id] = (title, text, line_widths, line_hpos)\n",
    "        texts_dict[issue_article_id] = (title, text, line_widths, line_hpos, line_offsets)\n",
    "\n",
    "    \n",
    "    return texts_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_block(block_strings):\n",
    "    \"\"\"\n",
    "    Given xml String elements from text block, return whole block\n",
    "    as single string.\n",
    "    \"\"\"\n",
    "    words = []\n",
    "    for s in block_strings:\n",
    "        words.append(s.attrib['CONTENT'])\n",
    "    total_string = ' '.join(words)\n",
    "\n",
    "    return total_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_block_lines(block_lines):\n",
    "    \"\"\"\n",
    "    Given xml TextLine elements from text block, return a list of the widths.\n",
    "    \"\"\"\n",
    "    line_w = []\n",
    "    \n",
    "    for line in block_lines:\n",
    "        line_w.append(int(line.attrib['WIDTH']))\n",
    "\n",
    "    return line_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_lines_hpos(block_lines):\n",
    "    \"\"\"\n",
    "    Given xml TextLine elements from text block, return a list of the\n",
    "    horizontal starting position of each line.\n",
    "    \"\"\"\n",
    "    line_hpos = []\n",
    "    \n",
    "    for line in block_lines:\n",
    "        line_hpos.append(int(line.attrib['HPOS']))\n",
    "\n",
    "    return line_hpos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_collect(filepath):\n",
    "    \"\"\"\n",
    "    Return dataframe for the selected newspaper/year.\n",
    "    \"\"\"\n",
    "    # print(f'Processing {path}')\n",
    "    try:\n",
    "        articles = process_tarball(filepath)\n",
    "        dataframe = pd.DataFrame.from_dict(\n",
    "            articles,\n",
    "            orient='index',\n",
    "            dtype = object,\n",
    "            # columns=['title', 'text', 'line_widths', 'line_hpos']\n",
    "            columns=['title', 'text', 'line_widths', 'line_hpos', 'line_offsets']\n",
    "            )\n",
    "    except:\n",
    "        print(f'Problem with {filepath}')\n",
    "        dataframe = None\n",
    "        \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code source: https://thispointer.com/python-how-to-get-list-of-files-in-directory-and-sub-directories/\n",
    "\n",
    "def get_files(dir_name):\n",
    "    \"\"\"\n",
    "    For the given path, get a list of all files in the directory tree\n",
    "    \"\"\"\n",
    "    # Create a list of files and sub directories \n",
    "    files_dir = os.listdir(dir_name)\n",
    "    file_list = list()\n",
    "    \n",
    "    # Iterate over all the entries\n",
    "    for item in files_dir:\n",
    "        \n",
    "        # Create full path\n",
    "        full_path = os.path.join(dir_name, item)\n",
    "        \n",
    "        # If entry is a directory then get the list of files in this directory \n",
    "        if os.path.isdir(full_path):\n",
    "            file_list = file_list + get_files(dir_name)\n",
    "        else:\n",
    "            file_list.append(full_path)\n",
    "                \n",
    "    return file_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_and_create(dir_name, num_issues):\n",
    "    \"\"\"\n",
    "    Randomly select a given number of issues\n",
    "    and return a dataframe.\n",
    "    \"\"\"\n",
    "\n",
    "    file_list = get_files(dir_name)\n",
    "    file_paths = []\n",
    "    single_dfs = [] # A list of the dataframes created for each newspaper issue\n",
    "\n",
    "    for random_selection in range(0, num_issues):\n",
    "        \n",
    "        selected_tar = random.choice(file_list)\n",
    "        file_paths.append(selected_tar)  \n",
    "    \n",
    "    for filepath in file_paths:\n",
    "        single_df = process_and_collect(filepath)\n",
    "        single_dfs.append(single_df)\n",
    "    \n",
    "    final_df = pd.concat(single_dfs, axis = 0)\n",
    "    final_df.reset_index(drop=False, inplace=True, col_level=0)\n",
    "        \n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_df(dir_name, num_issues):\n",
    "    \"\"\"\n",
    "    Given a directory for the Papers Past \n",
    "    open data (with newspaper-year files \n",
    "    in tar.gz format) and a number of issues \n",
    "    to randomly select, return a dataframe \n",
    "    of articles and features.\n",
    "    \"\"\"\n",
    "                \n",
    "    final_df = select_and_create(dir_name, num_issues)\n",
    "    \n",
    "    # Calculate features from the lists of line widths and positions\n",
    "    final_df['avg_line_width'] = pd.DataFrame(final_df['line_widths'].values.tolist()).mean(1)\n",
    "    final_df['max_line_width'] = pd.DataFrame(final_df['line_widths'].values.tolist()).max(1)\n",
    "    final_df['min_line_width'] = pd.DataFrame(final_df['line_widths'].values.tolist()).min(1)\n",
    "    final_df['line_width_range'] = final_df['max_line_width'] - final_df['min_line_width']\n",
    "    \n",
    "    # Line offsets relate to the difference between the starting horizontal position of each line compared to the block\n",
    "    final_df['avg_line_offset'] = pd.DataFrame(final_df['line_offsets'].values.tolist()).mean(1)\n",
    "    final_df['max_line_offset'] = pd.DataFrame(final_df['line_offsets'].values.tolist()).max(1)\n",
    "    final_df['min_line_offset'] = pd.DataFrame(final_df['line_offsets'].values.tolist()).min(1)\n",
    "       \n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem with E:/PapersPast_OpenData\\LT_1891.tar.gz\n",
      "Problem with E:/PapersPast_OpenData\\LT_1891.tar.gz\n",
      "Problem with E:/PapersPast_OpenData\\LT_1890.tar.gz\n",
      "Problem with E:/PapersPast_OpenData\\LT_1891.tar.gz\n",
      "Problem with E:/PapersPast_OpenData\\LT_1890.tar.gz\n",
      "Problem with E:/PapersPast_OpenData\\LT_1890.tar.gz\n",
      "Problem with E:/PapersPast_OpenData\\LT_1891.tar.gz\n",
      "Problem with E:/PapersPast_OpenData\\LT_1890.tar.gz\n",
      "Returned dataframe in 62180.5564 seconds\n"
     ]
    }
   ],
   "source": [
    "# Create the final dataframe and measure time to load\n",
    "t1 = time.perf_counter()\n",
    "\n",
    "final_df = produce_df(dir_name, num_issues)\n",
    "\n",
    "t2 = time.perf_counter()\n",
    "\n",
    "print(f\"Returned dataframe in {t2 - t1:0.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159417"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for and remove any duplicate articles\n",
    "\n",
    "final_df = final_df.drop_duplicates(subset='index', keep=\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrangled_df(final_df):\n",
    "    \"\"\"\n",
    "    Given the combined final dataframe of Papers Past articles, \n",
    "    rename and reorder columns, and add the full newspaper name from\n",
    "    a given dictionary supplied as a csv file.\n",
    "    \"\"\"\n",
    "    # A dictionary of newspaper codes mapped to newspaper name and region is loaded\n",
    "    codes2newspaper = pd.read_csv('PP_Codes2Newspaper.csv', header=None, dtype={0: str}).set_index(0).squeeze().to_dict()\n",
    "    \n",
    "    # Separate features are extracted from the 'index' column\n",
    "    final_df['newspaper_id'] = final_df[\"index\"].str.extract(r\"([^_]*)\") # Extract the letters before the first underscore as Newspaper ID\n",
    "    final_df['date'] = final_df[\"index\"].str.extract(r\"(?<=\\_)(.*?)(?=\\_)\") # Extract the numbers between the underscores as date\n",
    "    final_df['article_id'] = final_df[\"index\"].str.extract(r\"(\\d+)(?!.*\\d)\") # Extract the numeric portion of the article ID\n",
    "    final_df.drop('index', inplace=True, axis=1) # Drop the index column\n",
    "    \n",
    "    # The Northern Advocate's code is NA so it comes through as nan. This is deleted and then replaced correctly in the dictionary\n",
    "    codes2newspaper = {key: value for key, value in codes2newspaper.items() if pd.notna(key)}\n",
    "    codes2newspaper['NA'] = 'Northern Advocate' \n",
    "    final_df['newspaper'] = final_df['newspaper_id'].map(codes2newspaper)\n",
    "    \n",
    "    # The data types of the columns are updated\n",
    "    final_df['date'] = pd.to_datetime(final_df['date'], format='%Y%m%d') \n",
    "    final_df['article_id'] = (final_df['article_id']).astype(int)\n",
    "    final_df['text'] = (final_df['text']).astype('string')\n",
    "    final_df['title'] = (final_df['title']).astype('string')\n",
    "    \n",
    "    final_df['newspaper_id'] = (final_df['newspaper_id']).astype('string')\n",
    "    final_df['newspaper'] = (final_df['newspaper']).astype('string')\n",
    "    \n",
    "    # Columns are reordered\n",
    "    new_order = [\"date\", \"newspaper_id\", \"newspaper\", \"article_id\", \n",
    "                 \"avg_line_width\", \"min_line_width\", \"max_line_width\", \n",
    "                 \"line_width_range\", \"avg_line_offset\", \"max_line_offset\", \n",
    "                 \"min_line_offset\", \"title\", \"text\"]\n",
    "    clean_df = final_df.reindex(columns = new_order)\n",
    "    \n",
    "    return clean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>newspaper_id</th>\n",
       "      <th>newspaper</th>\n",
       "      <th>article_id</th>\n",
       "      <th>avg_line_width</th>\n",
       "      <th>min_line_width</th>\n",
       "      <th>max_line_width</th>\n",
       "      <th>line_width_range</th>\n",
       "      <th>avg_line_offset</th>\n",
       "      <th>max_line_offset</th>\n",
       "      <th>min_line_offset</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1891-12-23</td>\n",
       "      <td>WSTAR</td>\n",
       "      <td>Western Star</td>\n",
       "      <td>1</td>\n",
       "      <td>473.923529</td>\n",
       "      <td>71.0</td>\n",
       "      <td>496.0</td>\n",
       "      <td>425.0</td>\n",
       "      <td>5.252941</td>\n",
       "      <td>171.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>A Fool and A Woman.</td>\n",
       "      <td>Bayminster was in a flutter of excite ment. Ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1891-12-23</td>\n",
       "      <td>WSTAR</td>\n",
       "      <td>Western Star</td>\n",
       "      <td>2</td>\n",
       "      <td>325.100000</td>\n",
       "      <td>80.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>420.0</td>\n",
       "      <td>53.100000</td>\n",
       "      <td>216.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Shipping.</td>\n",
       "      <td>THE WEATHER. Saturday—Showery. Sunday—Fair. Mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1891-12-23</td>\n",
       "      <td>WSTAR</td>\n",
       "      <td>Western Star</td>\n",
       "      <td>3</td>\n",
       "      <td>410.500000</td>\n",
       "      <td>214.0</td>\n",
       "      <td>503.0</td>\n",
       "      <td>289.0</td>\n",
       "      <td>29.500000</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>MARRIAGE.</td>\n",
       "      <td>Haywood—Hitchcock.—On Dec. 16th, at the Manse,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1891-12-23</td>\n",
       "      <td>WSTAR</td>\n",
       "      <td>Western Star</td>\n",
       "      <td>4</td>\n",
       "      <td>444.000000</td>\n",
       "      <td>367.0</td>\n",
       "      <td>501.0</td>\n",
       "      <td>134.0</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>39.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>DEATH.</td>\n",
       "      <td>CAMPDKM,. —At Ermdaloon the 2flth Dec., Isabel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1891-12-23</td>\n",
       "      <td>WSTAR</td>\n",
       "      <td>Western Star</td>\n",
       "      <td>5</td>\n",
       "      <td>496.663462</td>\n",
       "      <td>253.0</td>\n",
       "      <td>525.0</td>\n",
       "      <td>272.0</td>\n",
       "      <td>19.788462</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>The Western Star. (PUBLISHED BI-WEEKLY.) WEDNE...</td>\n",
       "      <td>Thio whirligig of time rolls oh, am once again...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159412</th>\n",
       "      <td>1899-02-14</td>\n",
       "      <td>ST</td>\n",
       "      <td>Southland Times</td>\n",
       "      <td>19</td>\n",
       "      <td>528.000000</td>\n",
       "      <td>76.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>489.0</td>\n",
       "      <td>22.809859</td>\n",
       "      <td>258.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>ALLEGED INDECENT ASSAULT.</td>\n",
       "      <td>William James Staunton was charged with having...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159413</th>\n",
       "      <td>1899-02-14</td>\n",
       "      <td>ST</td>\n",
       "      <td>Southland Times</td>\n",
       "      <td>20</td>\n",
       "      <td>519.036364</td>\n",
       "      <td>114.0</td>\n",
       "      <td>563.0</td>\n",
       "      <td>449.0</td>\n",
       "      <td>41.236364</td>\n",
       "      <td>335.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>Education Board Election.</td>\n",
       "      <td>TO THE EDITOR. Sib, — Mr I. W. Raymond, in hie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159414</th>\n",
       "      <td>1899-02-14</td>\n",
       "      <td>ST</td>\n",
       "      <td>Southland Times</td>\n",
       "      <td>21</td>\n",
       "      <td>509.117647</td>\n",
       "      <td>193.0</td>\n",
       "      <td>551.0</td>\n",
       "      <td>358.0</td>\n",
       "      <td>29.264706</td>\n",
       "      <td>238.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>The Gore Prohibition Meeting.</td>\n",
       "      <td>TO THE EDITOR. Sib, — In your issue of 7th ins...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159415</th>\n",
       "      <td>1899-02-14</td>\n",
       "      <td>ST</td>\n",
       "      <td>Southland Times</td>\n",
       "      <td>22</td>\n",
       "      <td>530.781250</td>\n",
       "      <td>230.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>335.0</td>\n",
       "      <td>14.375000</td>\n",
       "      <td>43.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>Just a Minute Please!</td>\n",
       "      <td>Tired men, whether suffering from phy sical or...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159416</th>\n",
       "      <td>1899-02-14</td>\n",
       "      <td>ST</td>\n",
       "      <td>Southland Times</td>\n",
       "      <td>23</td>\n",
       "      <td>509.137339</td>\n",
       "      <td>52.0</td>\n",
       "      <td>561.0</td>\n",
       "      <td>509.0</td>\n",
       "      <td>19.012876</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Southland County Council</td>\n",
       "      <td>Mondat, 13th February. The ordinary meeting wa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>155619 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             date newspaper_id        newspaper  article_id  avg_line_width  \\\n",
       "0      1891-12-23        WSTAR     Western Star           1      473.923529   \n",
       "1      1891-12-23        WSTAR     Western Star           2      325.100000   \n",
       "2      1891-12-23        WSTAR     Western Star           3      410.500000   \n",
       "3      1891-12-23        WSTAR     Western Star           4      444.000000   \n",
       "4      1891-12-23        WSTAR     Western Star           5      496.663462   \n",
       "...           ...          ...              ...         ...             ...   \n",
       "159412 1899-02-14           ST  Southland Times          19      528.000000   \n",
       "159413 1899-02-14           ST  Southland Times          20      519.036364   \n",
       "159414 1899-02-14           ST  Southland Times          21      509.117647   \n",
       "159415 1899-02-14           ST  Southland Times          22      530.781250   \n",
       "159416 1899-02-14           ST  Southland Times          23      509.137339   \n",
       "\n",
       "        min_line_width  max_line_width  line_width_range  avg_line_offset  \\\n",
       "0                 71.0           496.0             425.0         5.252941   \n",
       "1                 80.0           500.0             420.0        53.100000   \n",
       "2                214.0           503.0             289.0        29.500000   \n",
       "3                367.0           501.0             134.0        26.000000   \n",
       "4                253.0           525.0             272.0        19.788462   \n",
       "...                ...             ...               ...              ...   \n",
       "159412            76.0           565.0             489.0        22.809859   \n",
       "159413           114.0           563.0             449.0        41.236364   \n",
       "159414           193.0           551.0             358.0        29.264706   \n",
       "159415           230.0           565.0             335.0        14.375000   \n",
       "159416            52.0           561.0             509.0        19.012876   \n",
       "\n",
       "        max_line_offset  min_line_offset  \\\n",
       "0                 171.0              0.0   \n",
       "1                 216.0              4.0   \n",
       "2                  40.0              0.0   \n",
       "3                  39.0              1.0   \n",
       "4                  30.0              0.0   \n",
       "...                 ...              ...   \n",
       "159412            258.0             -1.0   \n",
       "159413            335.0             25.0   \n",
       "159414            238.0             12.0   \n",
       "159415             43.0             -1.0   \n",
       "159416             98.0              0.0   \n",
       "\n",
       "                                                    title  \\\n",
       "0                                     A Fool and A Woman.   \n",
       "1                                               Shipping.   \n",
       "2                                               MARRIAGE.   \n",
       "3                                                  DEATH.   \n",
       "4       The Western Star. (PUBLISHED BI-WEEKLY.) WEDNE...   \n",
       "...                                                   ...   \n",
       "159412                          ALLEGED INDECENT ASSAULT.   \n",
       "159413                          Education Board Election.   \n",
       "159414                      The Gore Prohibition Meeting.   \n",
       "159415                              Just a Minute Please!   \n",
       "159416                           Southland County Council   \n",
       "\n",
       "                                                     text  \n",
       "0       Bayminster was in a flutter of excite ment. Ch...  \n",
       "1       THE WEATHER. Saturday—Showery. Sunday—Fair. Mo...  \n",
       "2       Haywood—Hitchcock.—On Dec. 16th, at the Manse,...  \n",
       "3       CAMPDKM,. —At Ermdaloon the 2flth Dec., Isabel...  \n",
       "4       Thio whirligig of time rolls oh, am once again...  \n",
       "...                                                   ...  \n",
       "159412  William James Staunton was charged with having...  \n",
       "159413  TO THE EDITOR. Sib, — Mr I. W. Raymond, in hie...  \n",
       "159414  TO THE EDITOR. Sib, — In your issue of 7th ins...  \n",
       "159415  Tired men, whether suffering from phy sical or...  \n",
       "159416  Mondat, 13th February. The ordinary meeting wa...  \n",
       "\n",
       "[155619 rows x 13 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clean_df = wrangled_df(final_df)\n",
    "display(clean_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df.to_pickle(f\"{export_filename}_SAMPLE.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <h3>Reload saved dataframe (if required)</h3>\n",
    "    <p>The previously saved dataframe of sample results and article layout features can be reloaded by uncommenting and running the following cell.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_df = pd.read_pickle(f\"{export_filename}_SAMPLE.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction: linguistic features and text statistics\n",
    "\n",
    "The following cells extract parts-of-speech and text statistic features and add them to the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaner(df, column_name):\n",
    "    \"\"\"\n",
    "    Remove unnecessary symbols to create a clean text column from the original dataframe column using a regex.\n",
    "    \"\"\"\n",
    "    # A column of sentence count is added to the dataframe before punctuation is removed.\n",
    "    df['sentence_count'] = df[column_name].apply(lambda x: textstat.sentence_count(x))\n",
    "\n",
    "    # Regex pattern for only alphanumeric, hyphenated text\n",
    "    pattern = re.compile(r\"[A-Za-z0-9\\-]{1,50}\")\n",
    "    df['clean_text'] = df[column_name].str.findall(pattern).str.join(' ')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = cleaner(clean_df, 'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_propn_spacy(doc):\n",
    "    \"\"\"\n",
    "    Given a Spacy doc object return counts of the \n",
    "    following parts-of-speech: proper nouns.\n",
    "    \n",
    "    Optional: uncomment the code lines to collect and \n",
    "    print a list of the tagged words.\n",
    "    \"\"\"\n",
    "    count_propn = 0\n",
    "    # propn_list = []\n",
    "\n",
    "    for token in doc:\n",
    "        if token.pos_ == 'PROPN':\n",
    "            count_propn += 1\n",
    "        \n",
    "    return count_propn \n",
    "\n",
    "\n",
    "def count_verb_spacy(doc):\n",
    "    \"\"\"\n",
    "    Given a Spacy doc object return counts of the \n",
    "    following parts-of-speech: verbs.\n",
    "    \n",
    "    Optional: uncomment the code lines to collect and \n",
    "    print a list of the tagged words.\n",
    "    \"\"\"\n",
    "    count_verb = 0\n",
    "    # verb_list = []\n",
    "\n",
    "    for token in doc:\n",
    "        if token.pos_ == 'VERB':\n",
    "            count_verb += 1\n",
    "\n",
    "            # verb_list.append(token)\n",
    "    # print(verb_list)\n",
    "\n",
    "    return count_verb\n",
    "\n",
    "\n",
    "def count_noun_spacy(doc):\n",
    "    \"\"\"\n",
    "    Given a Spacy doc object return counts of the \n",
    "    following parts-of-speech: nouns.\n",
    "    \n",
    "    Optional: uncomment the code lines to collect and \n",
    "    print a list of the tagged words.\n",
    "    \"\"\"\n",
    "    count_noun = 0\n",
    "    # noun_list = []\n",
    "\n",
    "    for token in doc:\n",
    "        if token.pos_ == 'NOUN':\n",
    "            count_noun += 1\n",
    "\n",
    "            # noun_list.append(token)\n",
    "    # print(noun_list)\n",
    "        \n",
    "    return count_noun\n",
    "\n",
    "\n",
    "def count_adj_spacy(doc):\n",
    "    \"\"\"\n",
    "    Given a Spacy doc object return counts of the \n",
    "    following parts-of-speech: adjectives.\n",
    "    \n",
    "    Optional: uncomment the code lines to collect and \n",
    "    print a list of the tagged words.\n",
    "    \"\"\"\n",
    "    count_adj = 0\n",
    "    # adj_list = []\n",
    "\n",
    "    for token in doc:\n",
    "        if token.pos_ == 'ADJ':\n",
    "            count_adj += 1\n",
    "\n",
    "            # adj_list.append(token)\n",
    "    # print(adj_list)\n",
    "        \n",
    "    return count_adj\n",
    "\n",
    "\n",
    "def count_nums_spacy(doc):\n",
    "    \"\"\"\n",
    "    Given a Spacy doc object return counts of the \n",
    "    following parts-of-speech: numbers.\n",
    "    \n",
    "    Optional: uncomment the code lines to collect and \n",
    "    print a list of the tagged words.\n",
    "    \"\"\" \n",
    "    count_nums = 0\n",
    "    # nums_list = []\n",
    "\n",
    "    for token in doc:\n",
    "        if token.pos_ == 'NUM':\n",
    "            count_nums += 1\n",
    "\n",
    "            # nums_list.append(token)\n",
    "    # print(nums_list)\n",
    "        \n",
    "    return count_nums\n",
    "\n",
    "\n",
    "def count_pron_spacy(doc):\n",
    "    \"\"\"\n",
    "    Given a Spacy doc object return counts of the \n",
    "    following parts-of-speech: pronouns.\n",
    "    \n",
    "    Optional: uncomment the code lines to collect and \n",
    "    print a list of the tagged words.\n",
    "    \"\"\"\n",
    "    count_pron = 0\n",
    "    # pron_list = []\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.pos_ == 'PRON':\n",
    "            count_pron += 1\n",
    "\n",
    "            # pron_list.append(token)\n",
    "    # print(pron_list)\n",
    "        \n",
    "    return count_pron\n",
    "\n",
    "\n",
    "def count_nnps_spacy(doc):\n",
    "    \"\"\"\n",
    "    Given a Spacy doc object return counts of the \n",
    "    following parts-of-speech: plural proper nouns.\n",
    "    \n",
    "    Optional: uncomment the code lines to collect and \n",
    "    print a list of the tagged words.\n",
    "    \"\"\"\n",
    "    count_nnps = 0\n",
    "    # nnps_list = []\n",
    "\n",
    "    for token in doc:\n",
    "        if token.tag_ == 'NNPS':\n",
    "            count_nnps += 1\n",
    "        \n",
    "    return count_nnps\n",
    "\n",
    "\n",
    "def count_vb_spacy(doc):\n",
    "    \"\"\"\n",
    "    Given a Spacy doc object return counts of the \n",
    "    following parts-of-speech: base form verbs.\n",
    "    \n",
    "    Optional: uncomment the code lines to collect and \n",
    "    print a list of the tagged words.\n",
    "    \"\"\"\n",
    "    count_vb = 0\n",
    "    # vb_list = []\n",
    "\n",
    "    for token in doc:\n",
    "        if token.tag_ == 'VB':\n",
    "            count_vb += 1\n",
    "\n",
    "            # vb_list.append(token)\n",
    "    # print(vb_list)\n",
    "\n",
    "    return count_vb\n",
    "\n",
    "\n",
    "def count_nn_spacy(doc):\n",
    "    \"\"\"\n",
    "    Given a Spacy doc object return counts of the \n",
    "    following parts-of-speech: singular or mass nouns.\n",
    "    \n",
    "    Optional: uncomment the code lines to collect and \n",
    "    print a list of the tagged words.\n",
    "    \"\"\"\n",
    "    count_nn = 0\n",
    "    # nn_list = []\n",
    "\n",
    "    for token in doc:\n",
    "        if token.tag_ == 'NN':\n",
    "            count_nn += 1\n",
    "\n",
    "            # nn_list.append(token)\n",
    "    # print(nn_list)\n",
    "        \n",
    "    return count_nn\n",
    "\n",
    "\n",
    "def count_jj_spacy(doc):\n",
    "    \"\"\"\n",
    "    Given a Spacy doc object return counts of the \n",
    "    following parts-of-speech: adjectives.\n",
    "    \n",
    "    Optional: uncomment the code lines to collect and \n",
    "    print a list of the tagged words.\n",
    "    \"\"\"\n",
    "    count_jj = 0\n",
    "    # jj_list = []\n",
    "\n",
    "    for token in doc:\n",
    "        if token.tag_ == 'JJ':\n",
    "            count_jj += 1\n",
    "\n",
    "            # jj_list.append(token)\n",
    "    # print(jj_list)\n",
    "        \n",
    "    return count_jj\n",
    "\n",
    "\n",
    "def count_cd_spacy(doc):\n",
    "    \"\"\"\n",
    "    Given a Spacy doc object return counts of the \n",
    "    following parts-of-speech: cardinal numbers.\n",
    "    \n",
    "    Optional: uncomment the code lines to collect and \n",
    "    print a list of the tagged words.\n",
    "    \"\"\" \n",
    "    count_cd = 0\n",
    "    # cd_list = []\n",
    "\n",
    "    for token in doc:\n",
    "        if token.tag_ == 'CD':\n",
    "            count_cd += 1\n",
    "\n",
    "            # nums_cd.append(token)\n",
    "    # print(cd_list)\n",
    "        \n",
    "    return count_cd\n",
    "\n",
    "\n",
    "def count_prp_spacy(doc):\n",
    "    \"\"\"\n",
    "    Given a Spacy doc object return counts of the \n",
    "    following parts-of-speech: personal pronouns.\n",
    "    \n",
    "    Optional: uncomment the code lines to collect and \n",
    "    print a list of the tagged words.\n",
    "    \"\"\"\n",
    "    count_prp = 0\n",
    "    # prp_list = []\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.tag_ == 'PRP':\n",
    "            count_prp += 1\n",
    "\n",
    "            # prp_list.append(token)\n",
    "    # print(prp_list)\n",
    "        \n",
    "    return count_prp\n",
    "\n",
    "\n",
    "def count_rb_spacy(doc):\n",
    "    \"\"\"\n",
    "    Given a Spacy doc object return counts of the \n",
    "    following parts-of-speech: adverbs.\n",
    "    \n",
    "    Optional: uncomment the code lines to collect and \n",
    "    print a list of the tagged words.\n",
    "    \"\"\"\n",
    "    count_rb = 0\n",
    "    # rb_list = []\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.tag_ == 'RB':\n",
    "            count_rb += 1\n",
    "\n",
    "            # rb_list.append(token)\n",
    "    # print(rb_list)\n",
    "        \n",
    "    return count_rb\n",
    "\n",
    "\n",
    "def count_cc_spacy(doc):\n",
    "    \"\"\"\n",
    "    Given a Spacy doc object return counts of the \n",
    "    following parts-of-speech: coordinating conjunctions.\n",
    "    \n",
    "    Optional: uncomment the code lines to collect and \n",
    "    print a list of the tagged words.\n",
    "    \"\"\"\n",
    "    count_cc = 0\n",
    "    # cc_list = []\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.tag_ == 'CC':\n",
    "            count_cc += 1\n",
    "\n",
    "            # cc_list.append(token)\n",
    "    # print(cc_list)\n",
    "        \n",
    "    return count_cc\n",
    "\n",
    "\n",
    "def count_nnp_spacy(doc):\n",
    "    \"\"\"\n",
    "    Given a Spacy doc object return counts of the \n",
    "    following parts-of-speech: singular proper nouns.\n",
    "    \n",
    "    Optional: uncomment the code lines to collect and \n",
    "    print a list of the tagged words.\n",
    "    \"\"\"\n",
    "    count_nnp = 0\n",
    "    # nnp_list = []\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.tag_ == 'NNP':\n",
    "            count_nnp += 1\n",
    "\n",
    "            # nnp_list.append(token)\n",
    "    # print(nnp_list)\n",
    "        \n",
    "    return count_nnp\n",
    "\n",
    "\n",
    "def count_vbd_spacy(doc):\n",
    "    \"\"\"\n",
    "    Given a Spacy doc object return counts of the \n",
    "    following parts-of-speech: past tense verbs.\n",
    "    \n",
    "    Optional: uncomment the code lines to collect and \n",
    "    print a list of the tagged words.\n",
    "    \"\"\"\n",
    "    count_vbd = 0\n",
    "    # vbd_list = []\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.tag_ == 'VBD':\n",
    "            count_vbd += 1\n",
    "\n",
    "            # vbd_list.append(token)\n",
    "    # print(vbd_list)\n",
    "        \n",
    "    return count_vbd\n",
    "\n",
    "\n",
    "def count_vbz_spacy(doc):\n",
    "    \"\"\"\n",
    "    Given a Spacy doc object return counts of the \n",
    "    following parts-of-speech: third-person singular present verbs.\n",
    "    \n",
    "    Optional: uncomment the code lines to collect and \n",
    "    print a list of the tagged words.\n",
    "    \"\"\"\n",
    "    count_vbz = 0\n",
    "    # vbz_list = []\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.tag_ == 'VBZ':\n",
    "            count_vbz += 1\n",
    "\n",
    "            # vbz_list.append(token)\n",
    "    # print(vbz_list)\n",
    "        \n",
    "    return count_vbz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_features_pipe(text_col, df):\n",
    "    \"\"\"\n",
    "    Process given text column of a dataframe to \n",
    "    extract linguistic features and add them to\n",
    "    the dataframe. Return the updated dataframe.\n",
    "    \"\"\"\n",
    "    \n",
    "    input_col = df[text_col]  \n",
    "    \n",
    "    propn_count = []\n",
    "    verb_count = []\n",
    "    noun_count = []\n",
    "    adj_count = []\n",
    "    nums_count = []\n",
    "    pron_count = []\n",
    "    \n",
    "    nnps_count = []\n",
    "    vb_count = []\n",
    "    nn_count = []\n",
    "    jj_count = []\n",
    "    cd_count = []\n",
    "    prp_count = []\n",
    "    rb_count = []\n",
    "    cc_count = []\n",
    "    nnp_count = []\n",
    "    vbd_count = []\n",
    "    vbz_count = []\n",
    "    \n",
    "    # spaCy processing pipeline\n",
    "    nlp_text_pipe = nlp.pipe(input_col, batch_size=20)\n",
    "    \n",
    "    for doc in nlp_text_pipe:\n",
    "        \n",
    "        # POS tags\n",
    "        # Universal POS Tags\n",
    "        # http://universaldependencies.org/u/pos/\n",
    "        \n",
    "        # Count proper nouns\n",
    "        propn_total = 0\n",
    "        count_propn = count_propn_spacy(doc)\n",
    "        propn_total += count_propn\n",
    "        propn_count.append(propn_total)\n",
    "        \n",
    "        # Count verbs\n",
    "        verb_total = 0\n",
    "        count_verb = count_verb_spacy(doc)\n",
    "        verb_total += count_verb\n",
    "        verb_count.append(verb_total)\n",
    "        \n",
    "        # Count nouns\n",
    "        noun_total = 0\n",
    "        count_noun = count_noun_spacy(doc)\n",
    "        noun_total += count_noun\n",
    "        noun_count.append(noun_total)\n",
    "        \n",
    "        # Count adjectives\n",
    "        adj_total = 0\n",
    "        count_adj = count_adj_spacy(doc)\n",
    "        adj_total += count_adj\n",
    "        adj_count.append(adj_total)\n",
    "        \n",
    "        # Count numbers\n",
    "        nums_total = 0\n",
    "        count_nums = count_nums_spacy(doc)\n",
    "        nums_total += count_nums\n",
    "        nums_count.append(nums_total)\n",
    "        \n",
    "        # Count pronouns\n",
    "        pron_total = 0\n",
    "        count_pron = count_pron_spacy(doc)\n",
    "        pron_total += count_pron\n",
    "        pron_count.append(pron_total)\n",
    "        \n",
    "        # POS tags (English)\n",
    "        # OntoNotes 5 / Penn Treebank\n",
    "        # https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "        \n",
    "        # Count plural proper nouns\n",
    "        nnps_total = 0\n",
    "        count_nnps = count_nnps_spacy(doc)\n",
    "        nnps_total += count_nnps\n",
    "        nnps_count.append(nnps_total)\n",
    "        \n",
    "        # Count base form verbs\n",
    "        vb_total = 0\n",
    "        count_vb = count_vb_spacy(doc)\n",
    "        vb_total += count_vb\n",
    "        vb_count.append(vb_total)\n",
    "        \n",
    "        # Count singular or mass nouns\n",
    "        nn_total = 0\n",
    "        count_nn = count_nn_spacy(doc)\n",
    "        nn_total += count_nn\n",
    "        nn_count.append(nn_total)\n",
    "        \n",
    "        # Count adjectives\n",
    "        jj_total = 0\n",
    "        count_jj = count_jj_spacy(doc)\n",
    "        jj_total += count_jj\n",
    "        jj_count.append(jj_total)\n",
    "        \n",
    "        # Count cardinal numbers\n",
    "        cd_total = 0\n",
    "        count_cd = count_cd_spacy(doc)\n",
    "        cd_total += count_cd\n",
    "        cd_count.append(cd_total)\n",
    "        \n",
    "        # Count personal pronouns\n",
    "        prp_total = 0\n",
    "        count_prp = count_prp_spacy(doc)\n",
    "        prp_total += count_prp\n",
    "        prp_count.append(prp_total)\n",
    "        \n",
    "        # Count adverbs\n",
    "        rb_total = 0\n",
    "        count_rb = count_rb_spacy(doc)\n",
    "        rb_total += count_rb\n",
    "        rb_count.append(rb_total)\n",
    "        \n",
    "        # Count coordinating conjunctions\n",
    "        cc_total = 0\n",
    "        count_cc = count_cc_spacy(doc)\n",
    "        cc_total += count_cc\n",
    "        cc_count.append(cc_total)\n",
    "        \n",
    "        # Count singular proper nouns\n",
    "        nnp_total = 0\n",
    "        count_nnp = count_nnp_spacy(doc)\n",
    "        nnp_total += count_nnp\n",
    "        nnp_count.append(nnp_total)\n",
    "        \n",
    "        # Count past tense verbs\n",
    "        vbd_total = 0\n",
    "        count_vbd = count_vbd_spacy(doc)\n",
    "        vbd_total += count_vbd\n",
    "        vbd_count.append(vbd_total)\n",
    "        \n",
    "        # Count third-person singular present verbs\n",
    "        vbz_total = 0\n",
    "        count_vbz = count_vbz_spacy(doc)\n",
    "        vbz_total += count_vbz\n",
    "        vbz_count.append(vbz_total)\n",
    "        \n",
    "    # Add features using the textstat library to the dataframe\n",
    "    # https://pypi.org/project/textstat/\n",
    "    df['word_count'] = input_col.apply(lambda x: textstat.lexicon_count(x, removepunct=True)) \n",
    "    df['syll_count'] = input_col.apply(lambda x: textstat.syllable_count(x))\n",
    "    df['polysyll_count'] = input_col.apply(lambda x: textstat.polysyllabcount(x)) # Returns the number of words with a syllable count greater than or equal to 3.\n",
    "    df['monosyll_count'] = input_col.apply(lambda x: textstat.monosyllabcount(x)) # Returns the number of words with a syllable count equal to one.\n",
    "    \n",
    "    # Add features using the textfeatures library to the dataframe\n",
    "    # https://towardsdatascience.com/textfeatures-library-for-extracting-basic-features-from-text-data-f98ba90e3932\n",
    "    tf.stopwords_count(df,text_col,'stopwords_count')\n",
    "    # tf.stopwords(df,text_col,'stopwords')  # Include a column that lists the stopwords found in the text\n",
    "    \n",
    "    try:\n",
    "        tf.avg_word_length(df,text_col,'avg_word_length')\n",
    "    except:\n",
    "        df['avg_word_length'] = 0\n",
    "    \n",
    "    try:\n",
    "        tf.char_count(df,text_col,'char_count')\n",
    "    except:\n",
    "        df['char_count'] = 0\n",
    "    \n",
    "    # Add features based on the spaCy pipeline to the dataframe\n",
    "    df['propn_count'] = propn_count\n",
    "    df['verb_count'] = verb_count\n",
    "    df['noun_count'] = noun_count\n",
    "    df['adj_count'] = adj_count\n",
    "    df['nums_count'] = nums_count\n",
    "    df['pron_count'] = pron_count\n",
    "    \n",
    "    df['nnps_count'] = nnps_count\n",
    "    df['vb_count'] = vb_count\n",
    "    df['nn_count'] = nn_count\n",
    "    df['jj_count'] = jj_count\n",
    "    df['cd_count'] = cd_count\n",
    "    df['prp_count'] = prp_count\n",
    "    df['rb_count'] = rb_count\n",
    "    df['cc_count'] = cc_count\n",
    "    df['nnp_count'] = nnp_count\n",
    "    df['vbd_count'] = vbd_count\n",
    "    df['vbz_count'] = vbz_count\n",
    "    \n",
    "    # Add frequency columns\n",
    "    \n",
    "    df['propn_freq'] = df['propn_count']/df['word_count']\n",
    "    df['verb_freq'] = df['verb_count']/df['word_count']\n",
    "    df['noun_freq'] = df['noun_count']/df['word_count']\n",
    "    df['adj_freq'] = df['adj_count']/df['word_count']\n",
    "    df['nums_freq'] = df['nums_count']/df['word_count']\n",
    "    df['pron_freq'] = df['pron_count']/df['word_count']\n",
    "    \n",
    "    df['nnps_freq'] = df['nnps_count']/df['word_count']\n",
    "    df['vb_freq'] = df['vb_count']/df['word_count']\n",
    "    df['nn_freq'] = df['nn_count']/df['word_count']\n",
    "    df['jj_freq'] = df['jj_count']/df['word_count']\n",
    "    df['cd_freq'] = df['cd_count']/df['word_count']\n",
    "    df['prp_freq'] = df['prp_count']/df['word_count']\n",
    "    df['rb_freq'] = df['rb_count']/df['word_count']\n",
    "    df['cc_freq'] = df['cc_count']/df['word_count']\n",
    "    df['nnp_freq'] = df['nnp_count']/df['word_count']\n",
    "    df['vbd_freq'] = df['vbd_count']/df['word_count']\n",
    "    df['vbz_freq'] = df['vbz_count']/df['word_count']\n",
    "    \n",
    "    df['polysyll_freq'] = df['polysyll_count']/df['word_count']\n",
    "    df['monosyll_freq'] = df['monosyll_count']/df['word_count']\n",
    "    df['stopword_freq'] = df['stopwords_count']/df['word_count']\n",
    "    \n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_col = 'clean_text'  # The name of dataframe column containing the text to be processed\n",
    "features_df = text_features_pipe(text_col, clean_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df.to_pickle(f\"{export_filename}_FEATURES.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <h3>Reload saved dataframe (if required)</h3>\n",
    "    <p>The previously saved dataframe of articles including all features can be reloaded by uncommenting and running the following cell.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features_df = pd.read_pickle(f\"{export_filename}_FEATURES.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply the saved model to the final dataset and export a csv file of results\n",
    "\n",
    "The following cells remove any rows with missing values, apply the saved model to the dataset, and display and export the final results in a dataframe sorted by probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = features_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df.reset_index(drop=True, inplace=True) # reset the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>newspaper_id</th>\n",
       "      <th>newspaper</th>\n",
       "      <th>article_id</th>\n",
       "      <th>avg_line_width</th>\n",
       "      <th>min_line_width</th>\n",
       "      <th>max_line_width</th>\n",
       "      <th>line_width_range</th>\n",
       "      <th>avg_line_offset</th>\n",
       "      <th>max_line_offset</th>\n",
       "      <th>...</th>\n",
       "      <th>cd_freq</th>\n",
       "      <th>prp_freq</th>\n",
       "      <th>rb_freq</th>\n",
       "      <th>cc_freq</th>\n",
       "      <th>nnp_freq</th>\n",
       "      <th>vbd_freq</th>\n",
       "      <th>vbz_freq</th>\n",
       "      <th>polysyll_freq</th>\n",
       "      <th>monosyll_freq</th>\n",
       "      <th>stopword_freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1891-12-23</td>\n",
       "      <td>WSTAR</td>\n",
       "      <td>Western Star</td>\n",
       "      <td>1</td>\n",
       "      <td>473.923529</td>\n",
       "      <td>71.0</td>\n",
       "      <td>496.0</td>\n",
       "      <td>425.0</td>\n",
       "      <td>5.252941</td>\n",
       "      <td>171.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007833</td>\n",
       "      <td>0.019147</td>\n",
       "      <td>0.040905</td>\n",
       "      <td>0.033072</td>\n",
       "      <td>0.084421</td>\n",
       "      <td>0.062663</td>\n",
       "      <td>0.007833</td>\n",
       "      <td>0.086162</td>\n",
       "      <td>0.728460</td>\n",
       "      <td>0.406440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1891-12-23</td>\n",
       "      <td>WSTAR</td>\n",
       "      <td>Western Star</td>\n",
       "      <td>2</td>\n",
       "      <td>325.100000</td>\n",
       "      <td>80.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>420.0</td>\n",
       "      <td>53.100000</td>\n",
       "      <td>216.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.011905</td>\n",
       "      <td>0.011905</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.547619</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.011905</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.178571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1891-12-23</td>\n",
       "      <td>WSTAR</td>\n",
       "      <td>Western Star</td>\n",
       "      <td>3</td>\n",
       "      <td>410.500000</td>\n",
       "      <td>214.0</td>\n",
       "      <td>503.0</td>\n",
       "      <td>289.0</td>\n",
       "      <td>29.500000</td>\n",
       "      <td>40.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.653846</td>\n",
       "      <td>0.230769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1891-12-23</td>\n",
       "      <td>WSTAR</td>\n",
       "      <td>Western Star</td>\n",
       "      <td>4</td>\n",
       "      <td>444.000000</td>\n",
       "      <td>367.0</td>\n",
       "      <td>501.0</td>\n",
       "      <td>134.0</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>39.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.105263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1891-12-23</td>\n",
       "      <td>WSTAR</td>\n",
       "      <td>Western Star</td>\n",
       "      <td>5</td>\n",
       "      <td>496.663462</td>\n",
       "      <td>253.0</td>\n",
       "      <td>525.0</td>\n",
       "      <td>272.0</td>\n",
       "      <td>19.788462</td>\n",
       "      <td>30.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006319</td>\n",
       "      <td>0.020537</td>\n",
       "      <td>0.071090</td>\n",
       "      <td>0.053712</td>\n",
       "      <td>0.033175</td>\n",
       "      <td>0.006319</td>\n",
       "      <td>0.036335</td>\n",
       "      <td>0.097946</td>\n",
       "      <td>0.703002</td>\n",
       "      <td>0.443918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154117</th>\n",
       "      <td>1899-02-14</td>\n",
       "      <td>ST</td>\n",
       "      <td>Southland Times</td>\n",
       "      <td>19</td>\n",
       "      <td>528.000000</td>\n",
       "      <td>76.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>489.0</td>\n",
       "      <td>22.809859</td>\n",
       "      <td>258.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022923</td>\n",
       "      <td>0.033429</td>\n",
       "      <td>0.025788</td>\n",
       "      <td>0.037249</td>\n",
       "      <td>0.167144</td>\n",
       "      <td>0.087870</td>\n",
       "      <td>0.004776</td>\n",
       "      <td>0.082139</td>\n",
       "      <td>0.762178</td>\n",
       "      <td>0.382044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154118</th>\n",
       "      <td>1899-02-14</td>\n",
       "      <td>ST</td>\n",
       "      <td>Southland Times</td>\n",
       "      <td>20</td>\n",
       "      <td>519.036364</td>\n",
       "      <td>114.0</td>\n",
       "      <td>563.0</td>\n",
       "      <td>449.0</td>\n",
       "      <td>41.236364</td>\n",
       "      <td>335.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010230</td>\n",
       "      <td>0.040921</td>\n",
       "      <td>0.040921</td>\n",
       "      <td>0.028133</td>\n",
       "      <td>0.109974</td>\n",
       "      <td>0.025575</td>\n",
       "      <td>0.028133</td>\n",
       "      <td>0.120205</td>\n",
       "      <td>0.693095</td>\n",
       "      <td>0.445013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154119</th>\n",
       "      <td>1899-02-14</td>\n",
       "      <td>ST</td>\n",
       "      <td>Southland Times</td>\n",
       "      <td>21</td>\n",
       "      <td>509.117647</td>\n",
       "      <td>193.0</td>\n",
       "      <td>551.0</td>\n",
       "      <td>358.0</td>\n",
       "      <td>29.264706</td>\n",
       "      <td>238.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004115</td>\n",
       "      <td>0.057613</td>\n",
       "      <td>0.049383</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.090535</td>\n",
       "      <td>0.041152</td>\n",
       "      <td>0.028807</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>0.748971</td>\n",
       "      <td>0.415638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154120</th>\n",
       "      <td>1899-02-14</td>\n",
       "      <td>ST</td>\n",
       "      <td>Southland Times</td>\n",
       "      <td>22</td>\n",
       "      <td>530.781250</td>\n",
       "      <td>230.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>335.0</td>\n",
       "      <td>14.375000</td>\n",
       "      <td>43.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008969</td>\n",
       "      <td>0.031390</td>\n",
       "      <td>0.026906</td>\n",
       "      <td>0.053812</td>\n",
       "      <td>0.174888</td>\n",
       "      <td>0.004484</td>\n",
       "      <td>0.035874</td>\n",
       "      <td>0.112108</td>\n",
       "      <td>0.654709</td>\n",
       "      <td>0.390135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154121</th>\n",
       "      <td>1899-02-14</td>\n",
       "      <td>ST</td>\n",
       "      <td>Southland Times</td>\n",
       "      <td>23</td>\n",
       "      <td>509.137339</td>\n",
       "      <td>52.0</td>\n",
       "      <td>561.0</td>\n",
       "      <td>509.0</td>\n",
       "      <td>19.012876</td>\n",
       "      <td>98.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031486</td>\n",
       "      <td>0.015743</td>\n",
       "      <td>0.017632</td>\n",
       "      <td>0.026448</td>\n",
       "      <td>0.168136</td>\n",
       "      <td>0.054786</td>\n",
       "      <td>0.001889</td>\n",
       "      <td>0.090680</td>\n",
       "      <td>0.686398</td>\n",
       "      <td>0.382242</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>154122 rows × 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             date newspaper_id        newspaper  article_id  avg_line_width  \\\n",
       "0      1891-12-23        WSTAR     Western Star           1      473.923529   \n",
       "1      1891-12-23        WSTAR     Western Star           2      325.100000   \n",
       "2      1891-12-23        WSTAR     Western Star           3      410.500000   \n",
       "3      1891-12-23        WSTAR     Western Star           4      444.000000   \n",
       "4      1891-12-23        WSTAR     Western Star           5      496.663462   \n",
       "...           ...          ...              ...         ...             ...   \n",
       "154117 1899-02-14           ST  Southland Times          19      528.000000   \n",
       "154118 1899-02-14           ST  Southland Times          20      519.036364   \n",
       "154119 1899-02-14           ST  Southland Times          21      509.117647   \n",
       "154120 1899-02-14           ST  Southland Times          22      530.781250   \n",
       "154121 1899-02-14           ST  Southland Times          23      509.137339   \n",
       "\n",
       "        min_line_width  max_line_width  line_width_range  avg_line_offset  \\\n",
       "0                 71.0           496.0             425.0         5.252941   \n",
       "1                 80.0           500.0             420.0        53.100000   \n",
       "2                214.0           503.0             289.0        29.500000   \n",
       "3                367.0           501.0             134.0        26.000000   \n",
       "4                253.0           525.0             272.0        19.788462   \n",
       "...                ...             ...               ...              ...   \n",
       "154117            76.0           565.0             489.0        22.809859   \n",
       "154118           114.0           563.0             449.0        41.236364   \n",
       "154119           193.0           551.0             358.0        29.264706   \n",
       "154120           230.0           565.0             335.0        14.375000   \n",
       "154121            52.0           561.0             509.0        19.012876   \n",
       "\n",
       "        max_line_offset  ...   cd_freq  prp_freq   rb_freq   cc_freq  \\\n",
       "0                 171.0  ...  0.007833  0.019147  0.040905  0.033072   \n",
       "1                 216.0  ...  0.047619  0.011905  0.011905  0.035714   \n",
       "2                  40.0  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "3                  39.0  ...  0.105263  0.000000  0.000000  0.000000   \n",
       "4                  30.0  ...  0.006319  0.020537  0.071090  0.053712   \n",
       "...                 ...  ...       ...       ...       ...       ...   \n",
       "154117            258.0  ...  0.022923  0.033429  0.025788  0.037249   \n",
       "154118            335.0  ...  0.010230  0.040921  0.040921  0.028133   \n",
       "154119            238.0  ...  0.004115  0.057613  0.049383  0.037037   \n",
       "154120             43.0  ...  0.008969  0.031390  0.026906  0.053812   \n",
       "154121             98.0  ...  0.031486  0.015743  0.017632  0.026448   \n",
       "\n",
       "        nnp_freq  vbd_freq  vbz_freq  polysyll_freq  monosyll_freq  \\\n",
       "0       0.084421  0.062663  0.007833       0.086162       0.728460   \n",
       "1       0.547619  0.023810  0.035714       0.011905       0.642857   \n",
       "2       0.615385  0.000000  0.000000       0.000000       0.653846   \n",
       "3       0.368421  0.052632  0.000000       0.157895       0.578947   \n",
       "4       0.033175  0.006319  0.036335       0.097946       0.703002   \n",
       "...          ...       ...       ...            ...            ...   \n",
       "154117  0.167144  0.087870  0.004776       0.082139       0.762178   \n",
       "154118  0.109974  0.025575  0.028133       0.120205       0.693095   \n",
       "154119  0.090535  0.041152  0.028807       0.074074       0.748971   \n",
       "154120  0.174888  0.004484  0.035874       0.112108       0.654709   \n",
       "154121  0.168136  0.054786  0.001889       0.090680       0.686398   \n",
       "\n",
       "        stopword_freq  \n",
       "0            0.406440  \n",
       "1            0.178571  \n",
       "2            0.230769  \n",
       "3            0.105263  \n",
       "4            0.443918  \n",
       "...               ...  \n",
       "154117       0.382044  \n",
       "154118       0.445013  \n",
       "154119       0.415638  \n",
       "154120       0.390135  \n",
       "154121       0.382242  \n",
       "\n",
       "[154122 rows x 59 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genres_binary(df, model):\n",
    "    \"\"\"\n",
    "    Run the model, and return the dataframe\n",
    "    with appended predictions.\n",
    "    \"\"\"\n",
    "\n",
    "    X = df.filter(features, axis=1)\n",
    "    indices = df.index.values\n",
    "\n",
    "    y_pred = model.predict(X)\n",
    "    y_prob = model.predict_proba(X)\n",
    "    \n",
    "\n",
    "    # Add the predictions to a copy of the original dataframe\n",
    "    df_new = df.copy()\n",
    "    df_new.loc[indices,'pred'] = y_pred\n",
    "    df_new.loc[indices,'prob_0'] = y_prob[:,0]\n",
    "    df_new.loc[indices,'prob_1'] = y_prob[:,1]\n",
    "    \n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model from disk\n",
    "loaded_model = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_df = genres_binary(features_df, loaded_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\AppData\\Local\\Temp/ipykernel_2376/2165945189.py:5: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  + final_df[\"newspaper\"].str.replace(' ','-').str.lower().str.replace(\"'\", \"\").str.replace(\".\", \"\") \\\n"
     ]
    }
   ],
   "source": [
    "final_df = preds_df.filter([\"date\", \"newspaper_id\", \"newspaper\", \"article_id\", \"title\", \"text\", \"pred\", \"prob_1\"], axis=1)\n",
    "final_df[\"newspaper_web\"] = final_df[\"date\"].astype('string')\n",
    "final_df[\"newspaper_web\"] = final_df[\"newspaper_web\"].str.replace('-','/')\n",
    "final_df[\"newspaper_web\"] = \"https://paperspast.natlib.govt.nz/newspapers/\" \\\n",
    "                            + final_df[\"newspaper\"].str.replace(' ','-', regex = False).str.lower().str.replace(\"'\", \"\", regex = False).str.replace(\".\", \"\", regex = False) \\\n",
    "                            + \"/\" \\\n",
    "                            + final_df[\"newspaper_web\"]\n",
    "\n",
    "# This line of code will provide a dataframe with only the articles classified as the given genre\n",
    "# final_df = final_df[final_df[\"pred\"] == 1].sort_values(by=\"prob_1\", ascending=False)\n",
    "\n",
    "# This line of code will provide a dataframe with all articles - ranked by probability of being the given genre\n",
    "final_df = final_df.sort_values(by=\"prob_1\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the line below to view the final dataframe if required\n",
    "\n",
    "# display(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the dataframe of results to a CSV file \n",
    "final_df.to_csv(f\"{export_filename}_FINAL.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
